{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893b713e",
   "metadata": {},
   "source": [
    "# Transformer implementation\n",
    "\n",
    "The main four steps in transformer architecture are the following:\n",
    "1. Embedding each token/feature/word into a vector, stack these vector so they form an **input**.\n",
    "2. Attention mechanism - the core of the approach, it figures out the meaning of each token/feature/word based on the context.\n",
    "3. MLP (Multi-Layered Perceptron), or Feed-Forward Layer - stores the facts and some relations between them.\n",
    "4. Unembedding - transforms the output of the last layer back into token/feature/word space.\n",
    "\n",
    "## Attention mechanism\n",
    "### Notations\n",
    "\n",
    "Symbol     | Meaning \n",
    "---------  | ------- \n",
    "$d$        | The model size, or embedding/positional encoding size\n",
    "$d_k, d_v$ | The per-head key, query and value dimensions\n",
    "$N$        | The sequence length of an input sequence (context window)\n",
    "$H$        | The number of heads in multi-head attention layer\n",
    "$\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ | The input sequence after mapping each element into an embedding vector\n",
    "$\\mathbf{W}_k \\in \\mathbb{R}^{d \\times d_k}$ | The key weight matrix\n",
    "$\\mathbf{W}_q \\in \\mathbb{R}^{d \\times d_k}$ | The query weight matrix\n",
    "$\\mathbf{W}_v \\in \\mathbb{R}^{d \\times d_v}$ | The value weight matrix\n",
    "$\\mathbf{W}_o \\in \\mathbb{R}^{d_v \\times d}$ | The output weight matrix.\n",
    "$\\mathbf{K} = \\mathbf{X} \\mathbf{W}_k,~\\mathbf{K} \\in \\mathbb{R}^{N \\times d_k}$ | The key embedding input, maps input to a key\n",
    "$\\mathbf{Q} = \\mathbf{X} \\mathbf{W}_q,~\\mathbf{Q} \\in \\mathbb{R}^{N \\times d_k}$ | The query embedding input, maps input to query\n",
    "$\\mathbf{V} = \\mathbf{X} \\mathbf{W}_v,~\\mathbf{V} \\in \\mathbb{R}^{d_k \\times d}$ | The value embedding input\n",
    "$\\mathbf{A} = softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\right), \\mathbf{A} \\in \\mathbb{R}^{N \\times N}$ | The self-attention matrix\n",
    "$\\mathbf{a}_{ij} \\in R$ | The dot-product between $\\mathbf{Q}_i$ and $\\mathbf{K}_j$\n",
    "\n",
    "\n",
    "Internally, a single attention-mechanism cycle uses the following data/brains to process the input:\n",
    "- Key weight matrix $\\mathbf{W}_k \\in \\mathbb{R}^{d \\times d_k}$.\n",
    "- Query weight matrix $\\mathbf{W}_q \\in \\mathbb{R}^{d \\times d_k}$.\n",
    "- Values weight matrix $\\mathbf{W}_v \\in \\mathbb{R}^{d \\times d_v}$.\n",
    "\n",
    "**Note**: most DL code uses tensors shaped $(B, N, d)$, where $B$ is batch size, $N$ is sequence length and $d$ is the dimensionality of an embedding vector. So if you drop the batch, then features are in the last dimension.\n",
    "\n",
    "*Why is it a common approach?*\n",
    "- First, it matches `nn.Linear` expecting features in the last dim.\n",
    "- Second, batching and multi-head layout is cleaner: $(N, h, N, d_k) \\to (B, h, N, N)$.\n",
    "\n",
    "The processing pipeline looks as follows:\n",
    "1. Take the stacked input $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$.\n",
    "2. For each head calculate keys $\\mathbf{K}_i$ and queries $\\mathbf{Q}_i$ for an input token $\\mathbf{x}$ as:\n",
    "  - $\\mathbf{K}_i = \\mathbf{x} \\mathbf{W}_k,~\\mathbf{K}_i \\in \\mathbb{R}^{1 \\times d_k}$\n",
    "  - $\\mathbf{Q}_i = \\mathbf{x} \\mathbf{W}_q,~\\mathbf{Q}_i \\in \\mathbb{R}^{1 \\times d_k}$\n",
    "3. Calculate a dot-product between each $\\mathbf{K}_i$ and $\\mathbf{Q}_i$. Dot product there tells how close are the query and the key and measures the similarity betweek the query and its key. The attention meachanism learns how different terms/queries/keys are related.\n",
    "4. All those dot-products are kept in a single matrix $\\mathbf{Q}\\mathbf{K}^T \\in \\mathbb{R}^{N \\times N}$.\n",
    "5. In case of self-attention, the upper-diagonal part of the matrix is nulled: this step is known as masking, it acts as a switch, that prevents current token from knowing \"future\" tokens. Then each row is normalized by dividing its values by $\\sqrt{d_k}$. It works as a temperature in softmax - you counteract the growth in dot-product magnitude:\n",
    "$$\n",
    "\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "6. Apply softmax to transform this to probabilities (acts as normalization as well), and then multiply by $\\mathbf{W}_v$ to get the corrections to our input (but these correction live in the head-block space).\n",
    "$$\n",
    "\\mathbf{z} = softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\right) \\mathbf{V}, \\mathbf{z} \\in \\mathbb{R}^{N \\times d_v}.\n",
    "$$\n",
    "7. Then we apply output matrix $\\mathbf{W}_o \\in \\mathbb{R}^{d_v \\times d}$ that projects to model dimensionality (in case of multiple heads its shape is $(h d_v) \\times d$ since we concatenate heads - $(\\mathbf{H}_1, \\dots, \\mathbf{H}_h) \\to \\mathbf{H}_{cat} \\in \\mathbb{R}^{n\\times(h\\cdot d_v)}$:\n",
    "$$\n",
    "\\Delta \\mathbf{I} = \\mathbf{z} \\mathbf{W}_o,~\\Delta \\mathbf{I} \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "These steps complete attention mechanism, $\\Delta \\mathbf{I}$ is then added to an input of the layer and normalized.\n",
    "\n",
    "**Note:** in real implementations, $\\mathbf{W}_q$, $\\mathbf{W}_k$, $\\mathbf{W}_v$ matrices are often stacked for efficiency and have the same embedding dimension, then the output from them can be obtained in a single cycle by projecting intput through `nn.Linear(d, 3 * d_k)`. $\\mathbf{W}_o$ is then also implemented as `nn.Linear` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828f52f",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron, or Fully-connected Feed Forward Network\n",
    "After the input was processed in the attention sub-layer, the next step is to pass it through MLP layer.\n",
    "This step is much more straightforward and consists of three sequential operations:\n",
    "1. Apply first linear transformation: $\\mathbf{y}_1 = \\mathbf{x} \\mathbf{W}_1 + \\mathbf{b}_1$\n",
    "2. Use ReLU activation: $\\mathbf{a} = \\max(0, \\mathbf{y}_1)$\n",
    "3. Apply second linear transformation: $\\mathbf{y}_2 = \\mathbf{a} \\mathbf{W}_2 + \\mathbf{b_2}$.\n",
    "\n",
    "Inner-layer dimensionality can be selected arbitrary. In the case of GPT-3 it is $4 \\times 12288$, four times the number of dimensions in the embedding space. So the sizes per token are:\n",
    "- $\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times d_{ff}}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{d_{ff}}$\n",
    "- $\\mathbf{W}_2 \\in \\mathbb{R}^{d_{ff} \\times d}$, $\\mathbf{b}_2 \\in \\mathbb{R}^d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b856b2",
   "metadata": {},
   "source": [
    "## Multihead Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e442c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, q_dim, k_dim, v_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.q_dim = q_dim\n",
    "        self.k_dim = k_dim\n",
    "        self.v_dim = v_dim\n",
    "\n",
    "        self.Wq = nn.Parameter(torch.empty(self.num_heads, self.embed_dim, self.q_dim))\n",
    "        self.Wk = nn.Parameter(torch.empty(self.num_heads, self.embed_dim, self.k_dim))\n",
    "        self.Wv = nn.Parameter(torch.empty(self.num_heads, self.embed_dim, self.v_dim))\n",
    "        self.Wo = nn.Parameter(torch.empty(self.num_heads * self.v_dim, self.embed_dim))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "\n",
    "    def forward(self, X, mask=None, return_attention = False):\n",
    "        Q = torch.einsum('bne,hev->bhnv', X, self.Wq)\n",
    "        K = torch.einsum('bne,hev->bhnv', X, self.Wk)\n",
    "        V = torch.einsum('bne,hev->bhnv', X, self.Wv)\n",
    "\n",
    "        logits = Q @ K.permute(0, 1, 3, 2) / (self.k_dim ** 0.5)\n",
    "        A = torch.softmax(logits, dim = -1)\n",
    "\n",
    "        Z = A @ V\n",
    "        Y = torch.einsum(\"bhnv, hve -> bne\", \n",
    "                         Z,\n",
    "                         self.Wo.view(self.num_heads, self.v_dim, self.embed_dim))\n",
    "\n",
    "        if return_attention:\n",
    "            return Y, A\n",
    "        else:\n",
    "            return Y\n",
    "        \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean(dim=-1, keepdim=True)\n",
    "        var = X.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        X_hat = (X - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * X_hat + self.beta\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, q_dim, k_dim, v_dim, dim_feedforward, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.mha = MultiheadAttention(input_dim, num_heads, q_dim, k_dim, v_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, input_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.norm1 = LayerNorm(input_dim)\n",
    "        self.norm2 = LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Multi-head attention\n",
    "        attn_output = self.mha(X)\n",
    "        X = self.norm1(X + attn_output)  # Add & Norm\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(X)\n",
    "        X = self.norm2(X + ffn_output)   # Add & Norm\n",
    "\n",
    "        return X\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, num_heads, q_dim, k_dim, v_dim, dim_feedforward, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(input_dim, num_heads, q_dim, k_dim, v_dim, dim_feedforward, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "    \n",
    "def sinusoidal_positions_encoder(L, d):\n",
    "    import torch, math\n",
    "    pos = torch.arange(L).float().unsqueeze(1)        # [L,1]\n",
    "    i = torch.arange(d//2).float().unsqueeze(0)       # [1,d/2]\n",
    "    denom = torch.pow(10000, (2*i)/d)                 # [1,d/2]\n",
    "    angles = pos / denom                               # [L,d/2]\n",
    "    pe = torch.zeros(L, d)\n",
    "    pe[:, 0::2] = torch.sin(angles)\n",
    "    pe[:, 1::2] = torch.cos(angles)\n",
    "    return pe  # [L,d]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022359e3",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "The experiments block was borrowed from [there](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html).\n",
    "\n",
    "### Sequence to Sequence\n",
    "A Sequence-To-Sequence task represent a class of tasks where the input is given, and the task is to get the output of an arbitrary length. It can be machine translation or summarizaion problems.\n",
    "\n",
    "**Problem**: given a sequence of N numbers between 0 and $M$ the task is to reverse it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17bc82",
   "metadata": {},
   "source": [
    "As usual, the first thing is to create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a2493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, num_samples, min_length, max_length, num_classes):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            # Random length for each sample\n",
    "            seq_len = torch.randint(min_length, max_length + 1, (1,)).item()\n",
    "            seq = torch.randint(1, num_classes, (seq_len,))\n",
    "            target = torch.flip(seq, dims=[0])\n",
    "            \n",
    "            self.data.append(seq)\n",
    "            self.targets.append(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the same length in this batch\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    padded_seqs = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "    padded_targets = torch.zeros(len(targets), max_len, dtype=torch.long)\n",
    "    \n",
    "    for i, (seq, target) in enumerate(zip(sequences, targets)):\n",
    "        padded_seqs[i, :len(seq)] = seq\n",
    "        padded_targets[i, :len(target)] = target\n",
    "    \n",
    "    return padded_seqs, padded_targets\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim, num_encoder_layers, num_heads, q_dim, k_dim, v_dim,\n",
    "                  dim_feedforward, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_classes, embed_dim)\n",
    "\n",
    "        self.max_seq_length = 5000\n",
    "        pe = sinusoidal_positions_encoder(self.max_seq_length, embed_dim)\n",
    "        self.register_buffer('positional_encoding', pe)\n",
    "\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            input_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            q_dim=q_dim,\n",
    "            k_dim=k_dim,\n",
    "            v_dim=v_dim,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        if seq_len > self.max_seq_length:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds maximum {self.max_seq_length}\")\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = x + self.positional_encoding[:seq_len, :].unsqueeze(0)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output_net(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c4846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9103\n",
      "Test Loss: 1.6215\n",
      "Epoch 3, Loss: 1.4074\n",
      "Test Loss: 1.3452\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, data_loader, epochs, loss_function, optimizer, test_loader=None):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch, labels in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(batch)\n",
    "            loss = loss_function(preds.view(-1, preds.size(-1)), labels.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if epoch % 3 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss/len(data_loader):.4f}\")\n",
    "\n",
    "        if test_loader is not None and epoch % 3 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                total_test_loss = 0\n",
    "                for batch, labels in test_loader:\n",
    "                    preds = model(batch)\n",
    "                    loss = loss_function(preds.view(-1, preds.size(-1)), labels.view(-1))\n",
    "                    total_test_loss += loss.item()\n",
    "                print(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\n",
    "\n",
    "torch.manual_seed(15)\n",
    "\n",
    "dataset = partial(Seq2SeqDataset, min_length=3, max_length=15, num_classes=20)\n",
    "train_loader = DataLoader(dataset(40000), batch_size=128, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset(1000), batch_size=128, shuffle=False, drop_last=True, collate_fn=collate_fn)\n",
    "\n",
    "model = Model(num_classes=20, embed_dim=16, num_encoder_layers=4, num_heads=4,\n",
    "              q_dim=16, k_dim=16, v_dim=16, dim_feedforward=512,\n",
    "              dropout_rate=0.0)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "train(model, train_loader, 15, F.cross_entropy, optimizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n",
      "Input: [1, 11, 17, 10, 3, 6, 6, 9, 1, 7, 1, 3, 17, 16, 0]\n",
      "Target: [16, 17, 3, 1, 7, 1, 9, 6, 6, 3, 10, 17, 11, 1, 0]\n",
      "Predicted: [14, 17, 3, 1, 7, 1, 9, 6, 6, 3, 10, 17, 11, 1, 0]\n",
      "\n",
      "Input: [14, 10, 6, 8, 13, 13, 15, 18, 8, 12, 12, 7, 14, 0, 0]\n",
      "Target: [14, 7, 12, 12, 8, 18, 15, 13, 13, 8, 6, 10, 14, 0, 0]\n",
      "Predicted: [14, 14, 12, 12, 8, 18, 15, 13, 13, 8, 6, 10, 14, 0, 0]\n",
      "\n",
      "Input: [3, 7, 12, 9, 4, 11, 6, 14, 18, 9, 0, 0, 0, 0, 0]\n",
      "Target: [9, 18, 14, 6, 11, 4, 9, 12, 7, 3, 0, 0, 0, 0, 0]\n",
      "Predicted: [9, 8, 14, 6, 11, 4, 9, 12, 7, 3, 0, 0, 0, 0, 0]\n",
      "\n",
      "Input: [7, 1, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Target: [17, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Predicted: [17, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Input: [14, 14, 4, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Target: [19, 4, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Predicted: [19, 4, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = next(iter(test_loader))\n",
    "test_preds = model(test_data[0])\n",
    "test_pred_classes = test_preds.argmax(dim=-1)\n",
    "print(\"Sample predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Input: {test_data[0][i].tolist()}\")\n",
    "    print(f\"Target: {test_data[1][i].tolist()}\")\n",
    "    print(f\"Predicted: {test_pred_classes[i].tolist()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de704380",
   "metadata": {},
   "source": [
    "## Gotchas\n",
    "Few things that I have encountered while implementing while implementing transformers from scratch:\n",
    "- Initialization - without `xavier` initialization, the neural network was never able to converge. It is porbably due to the vanishing gradients or weak signals.\n",
    "- When we have sequences of different length in our batch, default collate fails since stacking requires equal shapes. We need to tell PyTorch explicitly how to combine them. The right solution though, is to implement padding and masking for sequences.\n",
    "- High learning rate and the absence of gradient clipping might lead to gradient explosion. During backpropagation, gradients can grow, so clipping helps there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-ai-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
