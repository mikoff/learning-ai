{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "893b713e",
   "metadata": {},
   "source": [
    "# Transformer implementation\n",
    "\n",
    "The main four steps in transformer architecture are the following:\n",
    "1. Embedding each token/feature/word into a vector, stack these vector so they form an **input**.\n",
    "2. Attention mechanism - the core of the approach, it figures out the meaning of each token/feature/word based on the context.\n",
    "3. MLP (Multi-Layered Perceptron), or Feed-Forward Layer - stores the facts and some relations between them.\n",
    "4. Unembedding - transforms the output of the last layer back into token/feature/word space.\n",
    "\n",
    "## Attention mechanism\n",
    "### Notations\n",
    "\n",
    "Symbol     | Meaning \n",
    "---------  | ------- \n",
    "$d$        | The model size, or embedding/positional encoding size\n",
    "$d_k, d_v$ | The per-head key, query and value dimensions\n",
    "$N$        | The sequence length of an input sequence (context window)\n",
    "$H$        | The number of heads in multi-head attention layer\n",
    "$\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$ | The input sequence after mapping each element into an embedding vector\n",
    "$\\mathbf{W}_k \\in \\mathbb{R}^{d \\times d_k}$ | The key weight matrix\n",
    "$\\mathbf{W}_q \\in \\mathbb{R}^{d \\times d_k}$ | The query weight matrix\n",
    "$\\mathbf{W}_v \\in \\mathbb{R}^{d \\times d_v}$ | The value weight matrix\n",
    "$\\mathbf{W}_o \\in \\mathbb{R}^{d_v \\times d}$ | The output weight matrix.\n",
    "$\\mathbf{K} = \\mathbf{X} \\mathbf{W}_k,~\\mathbf{K} \\in \\mathbb{R}^{N \\times d_k}$ | The key embedding input, maps input to a key\n",
    "$\\mathbf{Q} = \\mathbf{X} \\mathbf{W}_q,~\\mathbf{Q} \\in \\mathbb{R}^{N \\times d_k}$ | The query embedding input, maps input to query\n",
    "$\\mathbf{V} = \\mathbf{X} \\mathbf{W}_v,~\\mathbf{V} \\in \\mathbb{R}^{d_k \\times d}$ | The value embedding input\n",
    "$\\mathbf{A} = softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\right), \\mathbf{A} \\in \\mathbb{R}^{N \\times N}$ | The self-attention matrix\n",
    "$\\mathbf{a}_{ij} \\in R$ | The dot-product between $\\mathbf{Q}_i$ and $\\mathbf{K}_j$\n",
    "\n",
    "\n",
    "Internally, a single attention-mechanism cycle uses the following data/brains to process the input:\n",
    "- Key weight matrix $\\mathbf{W}_k \\in \\mathbb{R}^{d \\times d_k}$.\n",
    "- Query weight matrix $\\mathbf{W}_q \\in \\mathbb{R}^{d \\times d_k}$.\n",
    "- Values weight matrix $\\mathbf{W}_v \\in \\mathbb{R}^{d \\times d_v}$.\n",
    "\n",
    "**Note**: most DL code uses tensors shaped $(B, N, d)$, where $B$ is batch size, $N$ is sequence length and $d$ is the dimensionality of an embedding vector. So if you drop the batch, then features are in the last dimension.\n",
    "\n",
    "*Why is it a common approach?*\n",
    "- First, it matches `nn.Linear` expecting features in the last dim.\n",
    "- Second, batching and multi-head layout is cleaner: $(N, h, N, d_k) \\to (B, h, N, N)$.\n",
    "\n",
    "The processing pipeline looks as follows:\n",
    "1. Take the stacked input $\\mathbf{X} \\in \\mathbb{R}^{N \\times d}$.\n",
    "2. For each head calculate keys $\\mathbf{K}_i$ and queries $\\mathbf{Q}_i$ for an input token $\\mathbf{x}$ as:\n",
    "  - $\\mathbf{K}_i = \\mathbf{x} \\mathbf{W}_k,~\\mathbf{K}_i \\in \\mathbb{R}^{1 \\times d_k}$\n",
    "  - $\\mathbf{Q}_i = \\mathbf{x} \\mathbf{W}_q,~\\mathbf{Q}_i \\in \\mathbb{R}^{1 \\times d_k}$\n",
    "3. Calculate a dot-product between each $\\mathbf{K}_i$ and $\\mathbf{Q}_i$. Dot product there tells how close are the query and the key and measures the similarity betweek the query and its key. The attention meachanism learns how different terms/queries/keys are related.\n",
    "4. All those dot-products are kept in a single matrix $\\mathbf{Q}\\mathbf{K}^T \\in \\mathbb{R}^{N \\times N}$.\n",
    "5. In case of self-attention, the upper-diagonal part of the matrix is nulled: this step is known as masking, it acts as a switch, that prevents current token from knowing \"future\" tokens. Then each row is normalized by dividing its values by $\\sqrt{d_k}$. It works as a temperature in softmax - you counteract the growth in dot-product magnitude:\n",
    "$$\n",
    "\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\n",
    "$$\n",
    "6. Apply softmax to transform this to probabilities (acts as normalization as well), and then multiply by $\\mathbf{W}_v$ to get the corrections to our input (but these correction live in the head-block space).\n",
    "$$\n",
    "\\mathbf{z} = softmax\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}} \\right) \\mathbf{V}, \\mathbf{z} \\in \\mathbb{R}^{N \\times d_v}.\n",
    "$$\n",
    "7. Then we apply output matrix $\\mathbf{W}_o \\in \\mathbb{R}^{d_v \\times d}$ that projects to model dimensionality (in case of multiple heads its shape is $(h d_v) \\times d$ since we concatenate heads - $(\\mathbf{H}_1, \\dots, \\mathbf{H}_h) \\to \\mathbf{H}_{cat} \\in \\mathbb{R}^{n\\times(h\\cdot d_v)}$:\n",
    "$$\n",
    "\\Delta \\mathbf{I} = \\mathbf{z} \\mathbf{W}_o,~\\Delta \\mathbf{I} \\in \\mathbb{R}^{n \\times d}\n",
    "$$\n",
    "\n",
    "These steps complete attention mechanism, $\\Delta \\mathbf{I}$ is then added to an input of the layer and normalized.\n",
    "\n",
    "**Note:** in real implementations, $\\mathbf{W}_q$, $\\mathbf{W}_k$, $\\mathbf{W}_v$ matrices are often stacked for efficiency and have the same embedding dimension, then the output from them can be obtained in a single cycle by projecting intput through `nn.Linear(d, 3 * d_k)`. $\\mathbf{W}_o$ is then also implemented as `nn.Linear` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1828f52f",
   "metadata": {},
   "source": [
    "## Multi-layer Perceptron, or Fully-connected Feed Forward Network\n",
    "After the input was processed in the attention sub-layer, the next step is to pass it through MLP layer.\n",
    "This step is much more straightforward and consists of three sequential operations:\n",
    "1. Apply first linear transformation: $\\mathbf{y}_1 = \\mathbf{x} \\mathbf{W}_1 + \\mathbf{b}_1$\n",
    "2. Use ReLU activation: $\\mathbf{a} = \\max(0, \\mathbf{y}_1)$\n",
    "3. Apply second linear transformation: $\\mathbf{y}_2 = \\mathbf{a} \\mathbf{W}_2 + \\mathbf{b_2}$.\n",
    "\n",
    "Inner-layer dimensionality can be selected arbitrary. In the case of GPT-3 it is $4 \\times 12288$, four times the number of dimensions in the embedding space. So the sizes per token are:\n",
    "- $\\mathbf{W}_1 \\in \\mathbb{R}^{d \\times d_{ff}}$, $\\mathbf{b}_1 \\in \\mathbb{R}^{d_{ff}}$\n",
    "- $\\mathbf{W}_2 \\in \\mathbb{R}^{d_{ff} \\times d}$, $\\mathbf{b}_2 \\in \\mathbb{R}^d$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b856b2",
   "metadata": {},
   "source": [
    "## Multihead Attention Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e442c553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, q_dim, k_dim, v_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.q_dim = q_dim\n",
    "        self.k_dim = k_dim\n",
    "        self.v_dim = v_dim\n",
    "\n",
    "        self.Wq = nn.Parameter(torch.empty(self.num_heads, self.embed_dim, self.q_dim))\n",
    "        self.Wk = nn.Parameter(torch.empty(self.num_heads, self.embed_dim, self.k_dim))\n",
    "        self.Wv = nn.Parameter(torch.empty(self.num_heads, self.embed_dim, self.v_dim))\n",
    "        self.Wo = nn.Parameter(torch.empty(self.num_heads * self.v_dim, self.embed_dim))\n",
    "        nn.init.xavier_uniform_(self.Wq)\n",
    "        nn.init.xavier_uniform_(self.Wk)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "\n",
    "    def forward(self, X, mask=None, return_attention = False):\n",
    "        Q = torch.einsum('bne,hev->bhnv', X, self.Wq)\n",
    "        K = torch.einsum('bne,hev->bhnv', X, self.Wk)\n",
    "        V = torch.einsum('bne,hev->bhnv', X, self.Wv)\n",
    "\n",
    "        logits = Q @ K.permute(0, 1, 3, 2) / (self.k_dim ** 0.5)\n",
    "        A = torch.softmax(logits, dim = -1)\n",
    "\n",
    "        Z = A @ V\n",
    "        Y = torch.einsum(\"bhnv, hve -> bne\", \n",
    "                         Z,\n",
    "                         self.Wo.view(self.num_heads, self.v_dim, self.embed_dim))\n",
    "\n",
    "        if return_attention:\n",
    "            return Y, A\n",
    "        else:\n",
    "            return Y\n",
    "        \n",
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_shape, eps=1e-5):\n",
    "        super().__init__()\n",
    "        self.normalized_shape = normalized_shape\n",
    "        self.eps = eps\n",
    "        self.gamma = nn.Parameter(torch.ones(normalized_shape))\n",
    "        self.beta = nn.Parameter(torch.zeros(normalized_shape))\n",
    "\n",
    "    def forward(self, X):\n",
    "        mean = X.mean(dim=-1, keepdim=True)\n",
    "        var = X.var(dim=-1, unbiased=False, keepdim=True)\n",
    "        X_hat = (X - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.gamma * X_hat + self.beta\n",
    "    \n",
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, q_dim, k_dim, v_dim, dim_feedforward, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.mha = MultiheadAttention(input_dim, num_heads, q_dim, k_dim, v_dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, input_dim),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "        self.norm1 = LayerNorm(input_dim)\n",
    "        self.norm2 = LayerNorm(input_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Multi-head attention\n",
    "        attn_output = self.mha(X)\n",
    "        X = self.norm1(X + attn_output)  # Add & Norm\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(X)\n",
    "        X = self.norm2(X + ffn_output)   # Add & Norm\n",
    "\n",
    "        return X\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers, input_dim, num_heads, q_dim, k_dim, v_dim, dim_feedforward, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderBlock(input_dim, num_heads, q_dim, k_dim, v_dim, dim_feedforward, dropout_rate)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "        return X\n",
    "    \n",
    "def sinusoidal_positions_encoder(L, d):\n",
    "    import torch, math\n",
    "    pos = torch.arange(L).float().unsqueeze(1)        # [L,1]\n",
    "    i = torch.arange(d//2).float().unsqueeze(0)       # [1,d/2]\n",
    "    denom = torch.pow(10000, (2*i)/d)                 # [1,d/2]\n",
    "    angles = pos / denom                               # [L,d/2]\n",
    "    pe = torch.zeros(L, d)\n",
    "    pe[:, 0::2] = torch.sin(angles)\n",
    "    pe[:, 1::2] = torch.cos(angles)\n",
    "    return pe  # [L,d]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022359e3",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "The experiments block was borrowed from [there](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html).\n",
    "\n",
    "### Sequence to Sequence\n",
    "A Sequence-To-Sequence task represent a class of tasks where the input is given, and the task is to get the output of an arbitrary length. It can be machine translation or summarizaion problems.\n",
    "\n",
    "**Problem**: given a sequence of N numbers between 0 and $M$ the task is to reverse it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d17bc82",
   "metadata": {},
   "source": [
    "As usual, the first thing is to create a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3a2493c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class Seq2SeqDataset(Dataset):\n",
    "    def __init__(self, num_samples, min_length, max_length, num_classes):\n",
    "        self.num_samples = num_samples\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.data = []\n",
    "        self.targets = []\n",
    "        \n",
    "        for _ in range(num_samples):\n",
    "            # Random length for each sample\n",
    "            seq_len = torch.randint(min_length, max_length + 1, (1,)).item()\n",
    "            seq = torch.randint(1, num_classes, (seq_len,))\n",
    "            target = torch.flip(seq, dims=[0])\n",
    "            \n",
    "            self.data.append(seq)\n",
    "            self.targets.append(target)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    sequences, targets = zip(*batch)\n",
    "    \n",
    "    # Pad sequences to the same length in this batch\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    \n",
    "    padded_seqs = torch.zeros(len(sequences), max_len, dtype=torch.long)\n",
    "    padded_targets = torch.zeros(len(targets), max_len, dtype=torch.long)\n",
    "    \n",
    "    for i, (seq, target) in enumerate(zip(sequences, targets)):\n",
    "        padded_seqs[i, :len(seq)] = seq\n",
    "        padded_targets[i, :len(target)] = target\n",
    "    \n",
    "    return padded_seqs, padded_targets\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes, embed_dim, num_encoder_layers, num_heads, q_dim, k_dim, v_dim,\n",
    "                  dim_feedforward, dropout_rate):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_classes, embed_dim)\n",
    "\n",
    "        self.max_seq_length = 5000\n",
    "        pe = sinusoidal_positions_encoder(self.max_seq_length, embed_dim)\n",
    "        self.register_buffer('positional_encoding', pe)\n",
    "\n",
    "\n",
    "        self.transformer_encoder = TransformerEncoder(\n",
    "            num_layers=num_encoder_layers,\n",
    "            input_dim=embed_dim,\n",
    "            num_heads=num_heads,\n",
    "            q_dim=q_dim,\n",
    "            k_dim=k_dim,\n",
    "            v_dim=v_dim,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout_rate=dropout_rate\n",
    "        )\n",
    "        self.output_net = nn.Sequential(\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(1)\n",
    "        \n",
    "        if seq_len > self.max_seq_length:\n",
    "            raise ValueError(f\"Sequence length {seq_len} exceeds maximum {self.max_seq_length}\")\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        x = x + self.positional_encoding[:seq_len, :].unsqueeze(0)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = self.output_net(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90c4846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9103\n",
      "Test Loss: 1.6215\n",
      "Epoch 3, Loss: 1.4074\n",
      "Test Loss: 1.3452\n"
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def train(model, data_loader, epochs, loss_function, optimizer, test_loader=None):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch, labels in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            preds = model(batch)\n",
    "            loss = loss_function(preds.view(-1, preds.size(-1)), labels.view(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        if epoch % 3 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss/len(data_loader):.4f}\")\n",
    "\n",
    "        if test_loader is not None and epoch % 3 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                total_test_loss = 0\n",
    "                for batch, labels in test_loader:\n",
    "                    preds = model(batch)\n",
    "                    loss = loss_function(preds.view(-1, preds.size(-1)), labels.view(-1))\n",
    "                    total_test_loss += loss.item()\n",
    "                print(f\"Test Loss: {total_test_loss/len(test_loader):.4f}\")\n",
    "\n",
    "torch.manual_seed(15)\n",
    "\n",
    "dataset = partial(Seq2SeqDataset, min_length=3, max_length=15, num_classes=20)\n",
    "train_loader = DataLoader(dataset(40000), batch_size=128, shuffle=True, drop_last=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(dataset(1000), batch_size=128, shuffle=False, drop_last=True, collate_fn=collate_fn)\n",
    "\n",
    "model = Model(num_classes=20, embed_dim=16, num_encoder_layers=4, num_heads=4,\n",
    "              q_dim=16, k_dim=16, v_dim=16, dim_feedforward=512,\n",
    "              dropout_rate=0.0)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "\n",
    "train(model, train_loader, 15, F.cross_entropy, optimizer, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5137867e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions:\n",
      "Input: [1, 11, 17, 10, 3, 6, 6, 9, 1, 7, 1, 3, 17, 16, 0]\n",
      "Target: [16, 17, 3, 1, 7, 1, 9, 6, 6, 3, 10, 17, 11, 1, 0]\n",
      "Predicted: [14, 17, 3, 1, 7, 1, 9, 6, 6, 3, 10, 17, 11, 1, 0]\n",
      "\n",
      "Input: [14, 10, 6, 8, 13, 13, 15, 18, 8, 12, 12, 7, 14, 0, 0]\n",
      "Target: [14, 7, 12, 12, 8, 18, 15, 13, 13, 8, 6, 10, 14, 0, 0]\n",
      "Predicted: [14, 14, 12, 12, 8, 18, 15, 13, 13, 8, 6, 10, 14, 0, 0]\n",
      "\n",
      "Input: [3, 7, 12, 9, 4, 11, 6, 14, 18, 9, 0, 0, 0, 0, 0]\n",
      "Target: [9, 18, 14, 6, 11, 4, 9, 12, 7, 3, 0, 0, 0, 0, 0]\n",
      "Predicted: [9, 8, 14, 6, 11, 4, 9, 12, 7, 3, 0, 0, 0, 0, 0]\n",
      "\n",
      "Input: [7, 1, 17, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Target: [17, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Predicted: [17, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Input: [14, 14, 4, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Target: [19, 4, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Predicted: [19, 4, 14, 14, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_data = next(iter(test_loader))\n",
    "test_preds = model(test_data[0])\n",
    "test_pred_classes = test_preds.argmax(dim=-1)\n",
    "print(\"Sample predictions:\")\n",
    "for i in range(5):\n",
    "    print(f\"Input: {test_data[0][i].tolist()}\")\n",
    "    print(f\"Target: {test_data[1][i].tolist()}\")\n",
    "    print(f\"Predicted: {test_pred_classes[i].tolist()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de704380",
   "metadata": {},
   "source": [
    "## Gotchas\n",
    "Few things that I have encountered while implementing while implementing transformers from scratch:\n",
    "- **Initialization** - without `xavier` initialization, the neural network was never able to converge. It is porbably due to the vanishing gradients or weak signals.\n",
    "- When there are sequences of different length in our batch, default **collate** fails since stacking requires equal shapes. It is neccessary to tell PyTorch explicitly how to combine them. The right solution though, is to implement padding and masking for sequences.\n",
    "- **High learning rate** and the absence of gradient clipping might lead to gradient explosion. During backpropagation, gradients can grow, so clipping helps there.\n",
    "- **Positional Encoding:** - needs to be non-learnable. The way to go is to register it as a buffer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-ai-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
