{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b7b2942",
   "metadata": {},
   "source": [
    "# Learn the basics\n",
    "My study notes and extracts while going through *PyTorch: Learn the Basics* [tutorial](https://docs.pytorch.org/tutorials/beginner/basics/intro.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b936397",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "- can share the same memory with Numpy arrays;\n",
    "- optimized for automaitic differentation;\n",
    "- a multi-dimensional array  with rank >= (0 - scalars, 1 - vectors, 2 - matrices). Don't confuse this with *matrix rank* in linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "253c1d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ea0092f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Tensor: \n",
      "tensor([[1, 2],\n",
      "        [3, 4]]) \n",
      "\n",
      "Ones Tensor: \n",
      "tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      "tensor([[0.1704, 0.2858],\n",
      "        [0.5805, 0.2223]]) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = np.array([[1, 2],[3, 4]])\n",
    "x_data = torch.tensor(data)\n",
    "x_ones = torch.ones_like(x_data)\n",
    "x_rand = torch.rand_like(x_data, dtype=torch.float)\n",
    "\n",
    "print(f\"Data Tensor: \\n{x_data} \\n\")\n",
    "print(f\"Ones Tensor: \\n{x_ones} \\n\")\n",
    "print(f\"Random Tensor: \\n{x_rand} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0dd9af",
   "metadata": {},
   "source": [
    "By default, tensors are allocated on CPU. If desired, they can be moved to the available accelerator using `.to` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "43e492f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4])\n",
      "cpu\n",
      "Accelerator is not available\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.zeros((2, 4))\n",
    "print(tensor.shape)\n",
    "print(tensor.device)\n",
    "\n",
    "if torch.accelerator.is_available():\n",
    "    print(\"Accelerator is available\")\n",
    "    tensor = tensor.to(torch.accelerator.current_accelerator())\n",
    "else:\n",
    "    print(\"Accelerator is not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc06cb2f",
   "metadata": {},
   "source": [
    "All usual operators, like indexing and slicing, are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a307a86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [0., 0., 0.],\n",
      "        [1., 1., 1.]])\n",
      "Concatenated tensors shape: torch.Size([4, 3])\n",
      "Stacked tensors shape: torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# slicing and indexing\n",
    "tensor = torch.ones(3, 3, 3)\n",
    "tensor[1, 1, :] = 0 # set the middle row of the middle matrix to zeros\n",
    "print(tensor[1, :, :])\n",
    "\n",
    "# concatenate tensors or stack\n",
    "tensor = torch.ones((2, 3))\n",
    "tensor_concat = torch.cat([tensor, tensor], dim = 0)\n",
    "tensor_stack = torch.stack([tensor, tensor], dim = 0)\n",
    "print(f\"Concatenated tensors shape: {tensor_concat.shape}\")\n",
    "print(f\"Stacked tensors shape: {tensor_stack.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364d45b1",
   "metadata": {},
   "source": [
    "In the same way as for Numpy, we use `*` for element-wise products of tensors and `@` for the matrix-like multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6900afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix multiplication shape: torch.Size([2, 2])\n",
      "Element-wise multiplication shape: torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "matrix_multiplication = tensor @ tensor.T  # matrix multiplication\n",
    "element_wise_multiplication = tensor * tensor   # element-wise multiplication\n",
    "print(f\"Matrix multiplication shape: {matrix_multiplication.shape}\")\n",
    "print(f\"Element-wise multiplication shape: {element_wise_multiplication.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe353f7",
   "metadata": {},
   "source": [
    "To calculate some aggregated statistics we can use the available methods and use `.item` to convert it to a numerical value. Underscore `_` means that the operand is in-place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a0260856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: <class 'torch.Tensor'>, Sum item: <class 'float'>\n",
      "Scalar tensor item: 11.0\n"
     ]
    }
   ],
   "source": [
    "sum = tensor.sum()\n",
    "print(f\"Sum: {type(sum)}, Sum item: {type(sum.item())}\")\n",
    "\n",
    "scalar_tensor = torch.ones(1)\n",
    "scalar_tensor.add_(10)\n",
    "print(f\"Scalar tensor item: {scalar_tensor.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce447a9a",
   "metadata": {},
   "source": [
    "If `.numpy()` method is used on a tensor, it returns a reference, so changing one will change the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dfe90f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scalar tensor numpy item: 11.0\n",
      "After in-place addition, scalar tensor item: 0.0\n",
      "After in-place addition, scalar tensor numpy item: 0.0\n"
     ]
    }
   ],
   "source": [
    "scalar_tensor_np = scalar_tensor.numpy()\n",
    "print(f\"Scalar tensor numpy item: {scalar_tensor_np.item()}\")\n",
    "scalar_tensor_np[:] = 0\n",
    "print(f\"After in-place addition, scalar tensor item: {scalar_tensor.item()}\")\n",
    "print(f\"After in-place addition, scalar tensor numpy item: {scalar_tensor_np.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ec42ae",
   "metadata": {},
   "source": [
    "## Datasets & dataloaders\n",
    "We ideally want our data loading code to be decoupled from model training code. For that PyTorch provides two interfaces:\n",
    "- `torch.utils.data.Dataset` - stores samples and their corresponding labels. Has to define three core methods: `__init__`, `__len__` and `__get_item__`.\n",
    "- `torch.utils.data.DataLoader` - wraps iterable around the `Dataset`. Usually we want that to pass samples in \"minibatches\" and shuffle data at every epoch. This class abstracts this complexity.\n",
    "\n",
    "To prototype models several datasets are available from PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0885d6c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "faaeb3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([15, 1, 28, 28])\n",
      "Label batch shape: torch.Size([15])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAksAAAGLCAYAAADXvdYrAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjcsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvTLEjVAAAAAlwSFlzAAAPYQAAD2EBqD+naQAAZ4RJREFUeJztnXl4VdXV/1dATJjDEEYhEJklCspkCgJOSMEWbBRHsGprFVtqoVb7U6FoHUF4qWPrBEoFXwSr6EutAlosMqhgEcIQCBDGME8yn98flt21v8lduTfjvTffz/PwPGtn3XPOvmeffe5mr+9eOyEIgkAIIYQQQkiBVCrvChBCCCGERDMcLBFCCCGEGHCwRAghhBBiwMESIYQQQogBB0uEEEIIIQYcLBFCCCGEGHCwRAghhBBiwMESIYQQQogBB0uEEEIIIQYxPVjKycmRhIQEGTduXImdc/78+ZKQkCDz588vsXOS8GGbxh9s0/iDbRp/sE1tynyw9Prrr0tCQoIsXbq0rC9dLlxxxRWSkJAg99xzT3lXpdSI9zZt0aKFJCQkFPivdevW5V29UiHe21REZMuWLXLddddJcnKy1KpVS3784x/L+vXry7tapUa8t+mYMWMK7KNJSUnlXbVSg21adpxV5lesQMycOVMWLlxY3tUgxWTixIly6NAh728bN26UBx98UK688spyqhUpDocOHZK+ffvK/v375fe//71UqVJFJkyYIL1795Zly5ZJvXr1yruKpIi88MILUqNGDVeuXLlyOdaGlATR0KYcLJUSR48elZEjR8rvfvc7efjhh8u7OqQYDBo0KN/fHn30URERuemmm8q4NqQkeP7552Xt2rWyePFi6dq1q4iI9O/fXzp27Cjjx4+Xxx57rJxrSIpKZmam1K9fv7yrQUqQaGjTqNQsHT9+XB5++GG56KKLpHbt2lK9enXp1auXzJs3L+QxEyZMkNTUVKlatar07t1bVqxYke8zWVlZkpmZKXXr1pWkpCTp0qWLvPfee4XW58iRI5KVlSW7du0K+zs89dRTcvr0aRk1alTYx8Qz8dCmmr/+9a/SsmVLycjIKNLx8UAst+mMGTOka9eubqAkItKuXTu57LLL5O233y70+Hglltv0DEEQyIEDByQIgrCPiWfYpiVDVA6WDhw4IC+//LL06dNHnnzySRkzZozk5eVJv379ZNmyZfk+P2XKFJk0aZIMHz5cHnjgAVmxYoVceumlsmPHDveZb7/9Vnr06CGrVq2S+++/X8aPHy/Vq1eXQYMGyaxZs8z6LF68WNq3by/PPvtsWPXftGmTPPHEE/Lkk09K1apVI/ru8Uqst6nm66+/llWrVsmNN94Y8bHxRKy26enTp+Wbb76RLl265PN169ZNsrOz5eDBg+HdhDgjVttUk5aWJrVr15aaNWvKzTff7NWlIsI2LSGCMua1114LRCRYsmRJyM+cPHkyOHbsmPe3vXv3Bg0bNgxuu+0297cNGzYEIhJUrVo1yM3NdX9ftGhRICLBvffe6/522WWXBenp6cHRo0fd306fPh1kZGQErVu3dn+bN29eICLBvHnz8v1t9OjRYX3HzMzMICMjw5VFJBg+fHhYx8YiFaFNNSNHjgxEJFi5cmXEx8YK8dymeXl5gYgEY8eOzed77rnnAhEJsrKyzHPEIvHcpkEQBBMnTgzuueeeYOrUqcGMGTOCESNGBGeddVbQunXrYP/+/YUeH4uwTcuOqBwsaU6dOhXs3r07yMvLCwYMGBB06tTJ+c407g033JDvuO7duwdt27YNgiAIdu/eHSQkJASPPPJIkJeX5/37wx/+EIiIezgKatxImDt3bpCQkBAsXrzY/Y2DJZ9Ya1Ose9OmTYPOnTsX+1zRTDy36aZNmwIRCZ588sl8vldeeSUQkeDrr7+O+LzRTjy3aSimTp0aiEjw+OOPl9g5owm2adkRlWE4EZHJkyfL+eefL0lJSVKvXj1JSUmRDz74QPbv35/vswUt327Tpo3k5OSIiMi6deskCAJ56KGHJCUlxfs3evRoERHZuXNnset88uRJ+dWvfiW33HKLp4Ug3xOLbYp8+umnsmXLFgq7/0MstumZ0PixY8fy+Y4ePep9piISi20aihtvvFEaNWokH3/8caldIxZgmxafqFwN9+abb8qtt94qgwYNkt/+9rfSoEEDqVy5sjz++OOSnZ0d8flOnz4tIiKjRo2Sfv36FfiZVq1aFavOIt/HelevXi0vvfSSe7DOcPDgQcnJyZEGDRpItWrVin2tWCNW2xSZOnWqVKpUSW644YYSP3esEattWrduXUlMTJRt27bl8535W5MmTYp9nVgkVtvUolmzZrJnz55SvUY0wzYtGaJysDRjxgxJS0uTmTNnSkJCgvv7mVErsnbt2nx/W7NmjbRo0UJEvheHiYhUqVJFLr/88pKv8H/YtGmTnDhxQn7wgx/k802ZMkWmTJkis2bNKnAperwTq22qOXbsmLzzzjvSp0+fCvtjqonVNq1UqZKkp6cXmMhv0aJFkpaWJjVr1iy160czsdqmoQiCQHJycqRz585lfu1ogW1aMkRlGO5MwqlALRNctGhRyASP7777rmzZssWVFy9eLIsWLZL+/fuLiEiDBg2kT58+8tJLLxX4v8m8vDyzPuEudbz++utl1qxZ+f6JiPzwhz+UWbNmSffu3c1zxCux2qaaDz/8UPbt28cQ3H+I5TbNzMyUJUuWeAOm1atXy9y5c+Xaa68t9Ph4JZbbtKBzvfDCC5KXlydXXXVVocfHK2zTkqHcZpZeffVVmTNnTr6/jxgxQgYOHCgzZ86UwYMHy4ABA2TDhg3y4osvSocOHfJlUhb5fsqvZ8+ectddd8mxY8dk4sSJUq9ePbnvvvvcZ5577jnp2bOnpKeny89+9jNJS0uTHTt2yMKFCyU3N1eWL18esq6LFy+Wvn37yujRo2XMmDEhP9euXTtp165dgb6WLVvG/YxSPLapZurUqZKYmCg/+clPwvp8PBCvbXr33XfLX/7yFxkwYICMGjVKqlSpIs8884w0bNhQRo4cGf4NikHitU1TU1NlyJAhkp6eLklJSbJgwQKZNm2adOrUSe68887wb1AMwjYtA8pUTh78V70f6t/mzZuD06dPB4899liQmpoaJCYmBp07dw5mz54dDBs2LEhNTXXnOqPef/rpp4Px48cHzZo1CxITE4NevXoFy5cvz3ft7OzsYOjQoUGjRo2CKlWqBE2bNg0GDhwYzJgxw32mpJeZB0HFWQ0Xz226f//+ICkpKbjmmmuKeptiiorQpps3bw4yMzODWrVqBTVq1AgGDhwYrF27tqi3LOqJ9za94447gg4dOgQ1a9YMqlSpErRq1Sr43e9+Fxw4cKA4ty2qYZuWHQlBwDSnhBBCCCGhiErNEiGEEEJItMDBEiGEEEKIAQdLhBBCCCEGHCwRQgghhBhwsEQIIYQQYsDBEiGEEEKIAQdLhBBCCCEGYWfw1nvKRANnUq+LfL8NhWbBggXOxnTsSUlJXlnvAdWmTRvPl5ub6+xo26akJNJjRVubVnTivU3PbLtwhurVqzv7zH5Todi8ebOz9+3b5/lSU1OdjRmBn3/+eWd/8cUXnu/ss8/2ysePHzfrUBTivU0rImzT+COcNuXMEiGEEEKIAQdLhBBCCCEGHCwRQgghhBiErVkqD+666y5n//73v/d855xzjrPXrl3r+S666CJnV61a1bzGsWPHnL1p0ybP16RJE2djTFPrH9555x3PN27cuJDXw1g1t+YjsY5+prUGUESkRo0azq5WrZrn279/v7N37drl+bp27eqVtUZR930RkfPPP9/ZWq8o4vfTRo0aeb4TJ06YZc3Ro0edXRraJkJIdMOZJUIIIYQQAw6WCCGEEEIMEoIw40BFXep41ll+pO/kyZPOHjBggOd74YUXvLKewtdT9iIiBw4ccDZOn+spffx6LVq08Mo6hHf69GnP17p1a2fjsufatWsXWE8RkS1btnjlgQMHOnvdunWeT98ffW8Kg8tX449obVPsw/rZF/HDa6dOnfJ8hw4dcjb20ypVqjj7yJEjng/7gg6JY/qP9evXh1V3rFtiYqJX1qkErPeWfvcURrS2KSk6bNP4g6kDCCGEEEKKCQdLhBBCCCEGHCwRQgghhBiUimapUqX/jsFQB6T58ssvvTJqf7Zv3+5s1BfoautlvVjXOnXqeL709HSvvGTJkpB11ToFra8Q8XUT3333nefDpc1Lly519uDBgyUUkaQVYNw8/oimNtXnady4sedD7Y9+/lGXpPVM+P30cdi/tdZJxNbzaT0hfn/UHlnoY/E76v5/+PDhsM8ZTW1KSoaK1qbh/p6XFXqbpJ///Oee79VXX3U26pwtqFkihBBCCCkmHCwRQgghhBiUSgZvPfWN2W71suOUlBTPh1l89XQbhtN2797tbD1NKOJn5cYlyTt27PDKun4YCtDl5ORkz6enUTGtQE5Ojldu3769szHD8cGDB0OeJ5JUAoSUJDokjs8l9imdJR/D1fqz6NPvCZwyr1+/vlfW4T0M9ekwvBV2KyyEoL8nvlOiIfxASFmAz74uoy+S3yjdN3v16uX5dHodlNVgSF7/ZuOYYcSIEc4eO3Zs2HULB84sEUIIIYQYcLBECCGEEGLAwRIhhBBCiEGpaJasOKbW76BGCDUNejkf6hTq1q3rbL1NgYi/JHnnzp2eb8WKFV5Z16FBgwaeT+sUULOgNUvow2WIWguBqQv+9a9/ORuXKxNSXujUGNhPcem81giivkn3BewXuk/jcXl5eWHXVfdFfE/geQkhkRGJLql79+7OvuSSSzxfu3btnL1582bPp397sc9+8sknXrlPnz4h66bTnFx99dWe7/3337eqXiicWSKEEEIIMeBgiRBCCCHEoFTCcBa9e/d2NobdrClzTB2gp/5XrlwZ0odLHTHUpZcs79271/N16NDB2fXq1fN8esk/ZmPF76HLffv29Xw6DEdItKD7pu5PIiJNmjTxylu3bnU2Lt23zqNDfTidPnLkSK+sUwlg+g+dnuCtt97yfDrlx7Zt28SCITtSUdG/k9iH9VL9e++91/P179/fK+s0IrNnz/Z806dPd/ZHH33k+fQ1Cwv7bdy40dnXXHON5xs3bpyzMzIyPN+GDRucjXKccODMEiGEEEKIAQdLhBBCCCEGHCwRQgghhBiUimbJ2sG3ZcuWzrb0QyIijRo1cvaePXs8n9ZJaP2QiK89QF2U3gVdxF8GjZqKrKwsZzdv3tzzaQ0Fpi7AVAJam3H++edLKEpiN2tCikKtWrW8sn5mUetzwQUXeGXsmxqtJ9JpQ0R8DQH2L708WEQkLS0tZH169uzp7LVr13q+devWObuwFAiob9TobYp0ahJC4gH97OMWZVpne/HFF3u+5557ziv/7W9/K9L1I0lPoLc/0eMAEZGmTZs6G/u3TiVAzRIhhBBCSAnDwRIhhBBCiAEHS4QQQgghBmWuWerRo4ezjx496vlQ+9OiRQtnL1y40PPpmOOFF17o+bQ2AXNGVK9e3Str3cQ555zj+bTe4csvv/R8mZmZzkYd1Pbt272yjsfq3E2ERAuo7dO6BdTooLZP95tly5Z5vq5duzpb6xXR9/nnn3u+Z555xivr3Crr16/PV/9Q6Lrhd0S0H3MucSsiEs+gzlazevVqZ+vfSxF7CxH8Pde/g9b1CqN169bOxt93nQMuNzfX86Wmpjq7U6dOEV+XM0uEEEIIIQYcLBFCCCGEGJT5did6ev/AgQOer27duiGPa9OmjVe2dkXXU+j79u3zfPhZHZbDqUG9gzFOy+vvgVuh1K5d2ytjSgRSOCUVBjnvvPO88oMPPujsG264oUjnjEcwdK638MG0Aps2bfLKeluBb775xvPp/oXL+vUu5DfddJPnmzVrllfWIXucQtch+SFDhoS8hk5jIJJ/e6MXX3zR2fguwi2NCIknrLCYXp6v03SIiDz88MNeecyYMSHPWdTQG26vdNFFFzlbp/cR8dMN1ahRw/Pt2rXL2c2aNYu4HpxZIoQQQggx4GCJEEIIIcSAgyVCCCGEEIMy1yxVrVrV2SdOnPB8uAT/nXfecbZOVS7ib3GCac2tc6akpHjlvLw8Z6MuRm9xoOst4usfULOkt0IR8bdn0DooET+uqtO4V3QsjdKdd97plQcOHOiVn3jiCWffeuutnu/66693Ni47vfbaayOtZqHgs6Gvj1sFlCe4ZYlOF4DPPqYO0Et5cdsC/VnUCWhtH2oLcUsTrVH897//7fk6d+7sbFyurPVMW7Zs8Xx6KXFh6H66e/fusI+LJvSWFqhRi2SrJa1D+9///V/Pp1M83HXXXZFWkZQT+tmwtuv6v//7P8+HeiINvgusa+h3MR6nU4yI5O/joa6JqUK0dhi1y+HAmSVCCCGEEAMOlgghhBBCDEo9DIfTdDq0pUNpIiINGjTwyosXL3b2Nddc4/l0OA1DNjqkgEvQcWdxPVWH59FTehiy0dPy7733nufDkKEOMeD0X5cuXZw9f/58IQVz2WWXOfvyyy/3fDrTuoi/fFWH5EREFixY4Oxf/OIXnk/vWI0hm0jQ53n55Zc9nw7RRlMYDtFhOMzgjSG7P/7xj87GtBk61PXTn/7U87322mvO1suBRfJn5dfZ/L/99lvPp9MVYF2Tk5OdPXjwYM83e/ZsCQVKBDDNQFmCaQsiCZlpIlm6rZ/TESNGeD4d2sZ3+AUXXOBs/f4W8dtbxA7LaCJJ21BS96qiYd1/nTpkzpw5nm/cuHFeWWfMxwzaGvw91b+1+BuNu17oZ65OnTohr4F9WMt1UFoQDpxZIoQQQggx4GCJEEIIIcSAgyVCCCGEEINS1yx1797dK+uYMi7rRyZPnuzsBx54wPNprRFuv6B1Cxj/3LFjh1fWmia9hYmIH1dFn9ZCjB8/3vP96Ec/8sp6qS3G1C+++GJnl7VmCfVc4VJSO7CjTkVvN6G3txDx7w2mscf4tz5W65dERJYuXersiRMner4ePXo4Oycnx/Nt377dK+vltKih0mWsm07PX9T7H21oLYDWLIiILF++3NmoZ0pLS3M2piMYPXq0V547d66zV61a5fl69erlbNxuRS8X7t27t+dbs2aNV9bPGC5PLs90AZbuJhKNjk5bgX0Il1mPHDnS2ahR0/cJtS56m5hhw4Z5PtQshauhikR3RI1SyaP1a7hl0M6dO73yHXfc4Wx892qstkc9EW63pN/N+j0s4v/e4/ZKWj+JWrtw4MwSIYQQQogBB0uEEEIIIQalHobr2LGjV9bTZBiiQHTIDMNgegoflwjqTNh6ybFI/t3EdQgBr6Gn+HDaWvPZZ5+F9In46QJwGrNt27bmsaVJSYXTNJgaAXepfvDBB529bNkyz6dDZBgGa9WqlbM//vhjz4cZ0/V5MCyk0wXoKWMRP9RWWDZ1XR/8rH5uMButLuuwSLzQpk0br/z+++87+29/+5vnu/TSS5395JNPer7169d7Zd1vcOr93HPPLdAWEWnXrp2zMcUHlnVIqzT6RmlghZ0we/wrr7zi7IULF3o+TLmg32kYLtb9Dfu7rg9KMIYOHeqVp0yZErLupPigBMUKfYWbxgFDYkuWLPHK9957r7MxDKfPi7/9lg/LWnZjpR/q169fvvqfAVPKhANnlgghhBBCDDhYIoQQQggx4GCJEEIIIcSg1DVLqCHQWgBLB4TopcMivhYKUwfouCrqW3CJrJVKX+tLmjdv7vk+/fTTkMehLkmnGcDUBbh8PlrR2gStAxHxNQy4HH/GjBleWafAx21K9HkwTv3FF184G3eoR12QPg+mY9Dp+lHrpFMHaE2SSP5UBvp74PUttIZKb3UTrzRs2NDZuE2J3voINTN4b/Ty9caNG3u+7OxsZ6NmSmsYHn30Uc+H6SD0Fkq4XD4WwXQHWiOE21ChDkxvMaFTn4j4fQH1TFo/um3bNs/3+OOPe+Ubb7zR2e3btw9ZH9TJ6P6G+hp8n2tNC+rQdOoIPE5rWSdNmiSxSCTb21jo36yuXbt6vhUrVnjlf/3rX87u06eP59PvYuudiVqrlStXeuXOnTs7GzWxOjWLTlsi4v8OFyUVCGeWCCGEEEIMOFgihBBCCDHgYIkQQgghxKDUNUuo9dFxVExrjqnTNRj/1rFS3Eahf//+zn7nnXc8H8bY9bYGmDNE11VrVAqr65YtW7yy1qlgHFnHUcuapk2beuWrrrrK2Vgv1AlpdJ4hrQkSya/10bmV8Bpal4SaIa1vQ60b6i30eTHPlz4Wj9N1xVg8fhbzN4UCtVe6bpFo9mIVrQOaNWuW5/vggw+cjdud/Pvf//bKWpe0YcMGz9etWzdnN2vWzPP94Ac/cDY+i/j+0Xob3IqpKNsjlBR/+tOfvLLWG+G2LHv37nU2PsMXXHCBszHfHOaq0/cK9SX6HYpaH12fAwcOiIXWs6J+TOtW8Pr6moVt96Lf6dgX9fsPz6Ofjd/85jcFf4FSAjU7Gvz9sPIjWXkM8Z5a+iat78HfWjxOP386j5qIr1nC96n+zX7ooYc837x587yybhvcFumSSy5xttbdieTfCilSOLNECCGEEGLAwRIhhBBCiEGph+FatmzplfU0KU7F4TJ/jU6jLuIvz8dtSvS04fjx4z0fTrfqaUNcWqqn+6yQyYQJE7wyLom3dlcPN5xTGmBoTd9/nKbVbYXhM31v9NJNEZFdu3Z55UaNGoU8j15qiiETDd4zLOtr4HmsrUh0fbBu1rYliLVVir5XGHqIB6ytCnQaARGR/fv3OxtTegwZMsQr6607pk6d6vn0O2b16tWeTy8Bt3Yox3J5ht0Q/Y4S8Z9vnX5BxE+VgPdQP7MoR0B0eAXbRvcpvKf6HYqyBuyLup9g2FOD59H1KWxrDP2+xy10NJjSRd/zu+++2/OhzKKksUJtkWxhEklKk1DXw2ug/CQ9Pd0rL1iwwNkogdBh1+uuu87z6a1w3nrrLc83duzYcKotIn4YPiMjw/NNmzYt7PMUBGeWCCGEEEIMOFgihBBCCDHgYIkQQgghxKDUNUuYVt9KMY9Lgh977DFno77m7bffdjYug9Uxbty2AJfa5uTkOBuX0mtdlF66LCLy05/+1NkYU/3oo4+8sl6Sj/F3rE9Zgqnidbwf9TSW1kenVcDlyvhZrVNAfUGLFi2cbemiELyGPi9eQ38PS0OBx1mfteqDx+nvhdd48MEHQ54zVsDvq7cxwWddpxVAjRDqkjSot1m6dKmz8Z7qrTJwWTmitTB4DdQzliUPP/xwSB++T7SeCbd+0RpM3MIE779+p6IuauvWrc7GFB966ymt1SwI/U7HLaJ0W2Fb6GcK642aUJ2+AJ9NrZnbuHGjWdfyxNIl6bbBZ//NN9/0ytdff72zLU2edT3cdkwv+Rfx7zcu1f/1r3/t7Isvvtjz9e3b19mFaTnDTZeAWyjpsUdR4MwSIYQQQogBB0uEEEIIIQalEobToQ6cetchCr2sV8RfWijiZ+bFMJheSow7VuvpVlySilPxbdu2dTbubq3rh9PEeorxjjvu8HyvvPKKV9ZTk9bS1vr163s+XHZf0mDITIflLr/8cs9nhcF0e+P3wylVPfWOy46ta1iZt/GakXw2FDhlX9gSZY1+xq1w3ezZs8OqSyyB0/s6fGUtScbl6Rje0e2BmaGt50YvAcc2wyn8ktqlvSzBsJMOJ0VzaKmiE0mWbv3cDhs2zPPpVBGYJgdDrbrf3HXXXZ7vz3/+c8jrazB1AIbadCZ4zPZ97bXXOluHixEcM2BqICsMp98/GIYrbjoQziwRQgghhBhwsEQIIYQQYsDBEiGEEEKIQalolvRO36gR0hoG3OkalxNqzZBe5iniLzvGtAJaB4SaJbympUvSOgmMf2oNla6LSP407/qaqDHQuqTevXt7vnfeeUdKE0zbP3369JCf1fFn1IhoXVCnTp08n04HIOJvTYLn0foe1ProtkAtF+qLtGYI0wpYu6nr+uA1UPsUqt6FXUPrD/Qu3PGCpdHCVCF6STj2U9Qi6PuIz42+36h90p/FvofX0O+mWNQvkfjkggsucDbqapcsWeLsxo0bez7UnepUAs8884zn0ykAnnrqKc/3+eefh6zbV1995ZW1zrhHjx6eD38LNFqHhBolS/eM6N/zkt6yiDNLhBBCCCEGHCwRQgghhBiUShgOs3Zr9HQbhsRwqauecsTpdT1NjlPmenofp+xwyaae/sdMsfq8GGrTYHbfbdu2hTwPhox0CgS9Y3K0ocOgGBLVYIoHUrHA51tnmMZpcR3axBAdTsXrcD6+N/BYje6b+A5BiYDu/4Vl+yakOEQS5tWh+y+//NLz6WcWM6a3bt3aK48aNcrZixYt8nyZmZnOfu+99zzfE0884WxMT4CyixtuuMHZt956q+fT7wZM42GF1qzfcCvNAobdiwtnlgghhBBCDDhYIoQQQggx4GCJEEIIIcSgVDRLesdqjCnqpX3WsmoRfysSjD/qZf56N20RXwtR2JJkrWNA7YO1bYW+vl4OL5J/iaQ+L+qb9DXwexASa2CKjTp16jgbNYG6XxSmEcJjQ4H929IzIbpvYj8lpCTB7Xw2bdrkbPxd1L9h+Fuj+1th2/l07tzZ2YsXL/Z8Wk+EqTl0GhV9DhGRBx980Cv/5je/cfayZcskFJZGCcHvUbNmTWejfjY5OdnZJf17ypklQgghhBADDpYIIYQQQgw4WCKEEEIIMSgVzVLDhg2djRoGnT8F85785S9/8crbt293dteuXT2fPi/qEvT2J7169fJ89evX98qzZ892NuZS0nXFa+i6XX/99Z4P81vMmjXL2fidd+/e7WzUVxESa2A/qV69urPx2df5UjD/mQVqGHQZdRvhap0KOpaQ0gK3ItE59nCrJa3RwVxlessq1O9gn9LbhqBmSv9mfvvtt55P/w6+//77nu/VV1/1ypMnT5ZQWPmRIsHKn6S1hitXrizyNQqCM0uEEEIIIQYcLBFCCCGEGJTKvLPewgOnFPVUIC7tmzdvnlfOyspytg5lRQKG9nQ6AhF7eWNRwV2Srel9fQ9wySYhsYYOuyG4HF+H5Qpb8m+F0/SyZww96GvgViy4fNlKI8LtT0hJ8uKLL3plne4Gty3p3r27s6+55pqQ58Qwtw6tiYjs2bMn5Gd132jevLnn02HAp59+2vNNmDAhZH2Q4oTewj1P7969nd2hQwfPh/c8UjizRAghhBBiwMESIYQQQogBB0uEEEIIIQalvt0Jxvp1vBE1A1rrhGCM1UqXrq+JqQuWL18e8rhItkbQ9dFLK0Xyx1RPnDgR0rd161ZnW9+fkGghEj2P1ilZfRb1i9hvLV2STrmB/Ut/FuttpSugRomUJfr5xiXvuvzaa6+FPAf+fjRu3Ngr165d29nY33Rf0GlxRES++eabkNe0QO2u7v/F0S9Zxy5YsMDZGzduLPI1CoIzS4QQQgghBhwsEUIIIYQYlEoYTk+/HT582PNZO43rcBWCU29FnSa3jovknNZUIGZStY7T19Q7tBMSi1h9WC+PxnJhfU+/UzAFgQbPo9N26DCEiMiOHTu8cmJiYsi6YtoBQqINvRtEQeWyBuUpJYUVzp8xY0apXFOEM0uEEEIIISYcLBFCCCGEGHCwRAghhBBiUCqapa5duzobdwg+cuSIs1Gj06RJk5DnxC1DLN1CWaC3X8AYKmojtP6hfv36Ic+JW7EQEmtgf9daI60JwjIu68c+pc+LW59onRQep5dE41Jm3F5In7ektmYghMQHnFkihBBCCDHgYIkQQgghxKBUwnA33XSTsxs1auT5Dh065Oxu3bp5vk8//TTkOcs77IZYyxezsrK88iOPPOLs7Oxsz6fDC3gcIdGItcwf02bo5cuYRkSHz2rVqhX2Na30BJiVWy9fto4TEdm5c6ezrf5NCKl4cGaJEEIIIcSAgyVCCCGEEAMOlgghhBBCDBICbq9NCCGEEBISziwRQgghhBhwsEQIIYQQYsDBEiGEEEKIAQdLhBBCCCEGMT1YysnJkYSEBBk3blyJnXP+/PmSkJAg8+fPL7FzkvBhm8YfbNP4g20af7BNbcp8sPT6669LQkKCLF26tKwvXaZMnz5dLr74YqlevbokJydLRkaGzJ07t7yrVSqwTeOPitCm06ZNkwsvvFCSkpIkJSVFbr/9dtm1a1d5V6vUqAhtqrniiiskISFB7rnnnvKuSqlREdr0448/lr59+0r9+vUlOTlZunXrJm+88UaZ16NUtjup6IwZM0bGjh0rmZmZcuutt8qJEydkxYoVsmXLlvKuGikibNP44oUXXpC7775bLrvsMnnmmWckNzdX/ud//keWLl0qixYtkqSkpPKuIikGM2fOlIULF5Z3NUgxee+992TQoEFy8cUXy5gxYyQhIUHefvttGTp0qOzatUvuvffeMqsLB0slzBdffCFjx46V8ePHl2lDktKDbRpfHD9+XH7/+9/LJZdcIv/4xz8kISFBREQyMjLk6quvlr/85S/yy1/+spxrSYrK0aNHZeTIkfK73/1OHn744fKuDikGzz77rDRu3Fjmzp0riYmJIiJy5513Srt27eT1118v0/dxVGqWjh8/Lg8//LBcdNFFUrt2balevbr06tVL5s2bF/KYCRMmSGpqqlStWlV69+4tK1asyPeZrKwsyczMlLp160pSUpJ06dJF3nvvvULrc+TIEcnKygprin7ixInSqFEjGTFihARB4G0cXJFhm8YfsdqmK1askH379smQIUPcQElEZODAgVKjRg2ZNm1aodeKV2K1TTVPPfWUnD59WkaNGhX2MfFMLLfpgQMHpE6dOm6gJCJy1llnSf369aVq1aqFHl+SROVg6cCBA/Lyyy9Lnz595Mknn5QxY8ZIXl6e9OvXT5YtW5bv81OmTJFJkybJ8OHD5YEHHpAVK1bIpZdeKjt27HCf+fbbb6VHjx6yatUquf/++2X8+PFSvXp1GTRokMyaNcusz+LFi6V9+/by7LPPFlr3Tz75RLp27SqTJk2SlJQUqVmzpjRu3DisY+MZtmn8EatteuzYMRGRAl+2VatWla+//lpOnz4dxh2IP2K1Tc+wadMmeeKJJ+TJJ58s8x/TaCWW27RPnz7y7bffykMPPSTr1q2T7OxseeSRR2Tp0qVy3333RXwvikVQxrz22muBiARLliwJ+ZmTJ08Gx44d8/62d+/eoGHDhsFtt93m/rZhw4ZARIKqVasGubm57u+LFi0KRCS499573d8uu+yyID09PTh69Kj72+nTp4OMjIygdevW7m/z5s0LRCSYN29evr+NHj3a/G579uwJRCSoV69eUKNGjeDpp58Opk+fHlx11VWBiAQvvviieXyswjaNP+K5TfPy8oKEhITg9ttv9/6elZUViEggIsGuXbvMc8Qi8dymZ8jMzAwyMjJcWUSC4cOHh3VsLBLvbXro0KHguuuuCxISElzfrFatWvDuu+8WemxJE5WDJc2pU6eC3bt3B3l5ecGAAQOCTp06Od+Zxr3hhhvyHde9e/egbdu2QRAEwe7du4OEhITgkUceCfLy8rx/f/jDHwIRcQ9HQY0bLps2bXINOm3aNO87dOjQITjnnHMiPmcswDaNP+K5TYMgCIYMGRKcddZZwbhx44Ls7Ozgs88+Cy644IKgSpUqgYgEmzdvLtJ5o5l4b9O5c+cGCQkJweLFi93fOFjyibU2PXHiRPDggw8G1157bfDWW28Fb775ZnDJJZcENWrUCBYuXFikcxaVqAzDiYhMnjxZzj//fElKSpJ69epJSkqKfPDBB7J///58n23dunW+v7Vp00ZycnJERGTdunUSBIE89NBDkpKS4v0bPXq0iIjs3Lmz2HU+M+1bpUoVyczMdH+vVKmSDBkyRHJzc2XTpk3Fvk6swjaNP2KxTUVEXnrpJfnhD38oo0aNknPPPVcuueQSSU9Pl6uvvlpERGrUqFEi14lFYrFNT548Kb/61a/klltuka5duxb7fPFGLLapiMg999wj77//vkybNk2uv/56uemmm+Tjjz+Wxo0by4gRI0rkGuESlavh3nzzTbn11ltl0KBB8tvf/lYaNGgglStXlscff1yys7MjPt8Z/cGoUaOkX79+BX6mVatWxaqziDihW3JyslSuXNnzNWjQQERE9u7dK82bNy/2tWINtmn8EattKiJSu3Zt+dvf/iabNm2SnJwcSU1NldTUVMnIyJCUlBRJTk4ukevEGrHaplOmTJHVq1fLSy+95H7Uz3Dw4EHJycmRBg0aSLVq1Yp9rVgjVtv0+PHj8sorr8h9990nlSr9d16nSpUq0r9/f3n22Wfl+PHjcvbZZxf7WuEQlYOlGTNmSFpamsycOdNbrXJm1IqsXbs239/WrFkjLVq0EBGRtLQ0Efn+Jl9++eUlX+H/UKlSJenUqZMsWbIkXyNu3bpVRERSUlJK7frRDNs0/ojVNtU0b97cDXT37dsnX375pfzkJz8pk2tHI7Happs2bZITJ07ID37wg3y+KVOmyJQpU2TWrFkyaNCgUqtDtBKrbbp79245efKknDp1Kp/vxIkTcvr06QJ9pUVUhuHO/A8+CAL3t0WLFoVMMvbuu+96yQEXL14sixYtkv79+4vI9zMAffr0kZdeekm2bduW7/i8vDyzPpEsdRwyZIicOnVKJk+e7P529OhRmTp1qnTo0EGaNGlS6DniEbZp/BHLbVoQDzzwgJw8ebJC59KK1Ta9/vrrZdasWfn+iYj88Ic/lFmzZkn37t3Nc8QrsdqmDRo0kOTkZJk1a5YcP37c/f3QoUPy/vvvS7t27cp0xWO5zSy9+uqrMmfOnHx/HzFihAwcOFBmzpwpgwcPlgEDBsiGDRvkxRdflA4dOhSY46ZVq1bSs2dPueuuu+TYsWMyceJEqVevnre08LnnnpOePXtKenq6/OxnP5O0tDTZsWOHLFy4UHJzc2X58uUh67p48WLp27evjB49WsaMGWN+rzvvvFNefvllGT58uKxZs0aaN28ub7zxhmzcuFHef//98G9QDMI2jT/itU2feOIJWbFihXTv3l3OOusseffdd+Wjjz6SRx99NO41L/HYpu3atZN27doV6GvZsmXczyjFY5tWrlxZRo0aJQ8++KD06NFDhg4dKqdOnZJXXnlFcnNz5c0334zsJhWXMpWTB/9V74f6t3nz5uD06dPBY489FqSmpgaJiYlB586dg9mzZwfDhg0LUlNT3bnOqPeffvrpYPz48UGzZs2CxMTEoFevXsHy5cvzXTs7OzsYOnRo0KhRo6BKlSpB06ZNg4EDBwYzZsxwnymJ5as7duwIhg0bFtStWzdITEwMunfvHsyZM6eotyzqYZvGH/HeprNnzw66desW1KxZM6hWrVrQo0eP4O233y7OLYt64r1NC0IqyGq4eG7TqVOnBt26dQuSk5ODqlWrBt27d/euUVYkBIGamyOEEEIIIR5RqVkihBBCCIkWOFgihBBCCDHgYIkQQgghxICDJUIIIYQQAw6WCCGEEEIMOFgihBBCCDHgYIkQQgghxCDsDN56T5myQl8Tr39mM7+CuPnmm53dvn17z4ep2M877zxnjx8/3vNlZWU5W2/kh2CqqrJIXVUS1yiNNsVzYj179OjhbMzy+t133zn7rLP8R/PkyZMlVUVHly5dvPL69eudvWfPHs9X2PcqCaK1TUnRiZU2LerzfWYLjDPUqlXL2fv27fN8u3fv9sq6/+F7+Z133gm7ruFSUn02VtqUhE84bcqZJUIIIYQQAw6WCCGEEEIMwt7upDSmDatUqeKVT5w4UaTzYIhs48aNzl67dq15rP5e2dnZnu+OO+4oUn3O7PJ8Bn2LrfBhJETrVHBh0/nnnnuus/VO0iIimzdvLlLd8Br6WMuXnp7u+XQYrqANJkubaG1TUnSitU3xnRnJe+mRRx5x9q9//WvPt3LlSmcfO3bM8x08eNArt2zZ0tn4zvzTn/7k7GeffTbsuln3imE4EgqG4QghhBBCigkHS4QQQgghBuUahisMHabLzMz0fEOHDnU2TuE2atTI2W3atPF8iYmJXlmvetqyZYvn09PIb7/9tud77733nL169eqCv0ApUp5TwXi/NadOnTKPrVGjhrPPPvtsz4cr0EoD/Z0bNGjg+fRqHfweVtiC0/skFOXZpsUJSbVt29bZkydP9nx65RqG0vW7AUN7uKJV1w9XyqWlpTl76dKlnm/s2LHOPnr0aMFfoBRhP40/GIYjhBBCCCkmHCwRQgghhBhwsEQIIYQQYlCumqUf//jHXvnGG2/0yk2aNHF27dq1PZ/WlOCS1MOHDzu7Xbt2ng8zQ2styqeffur56tSp4+y6deuGvAZqnbZt2+aV77vvPgmFtczdIlbj5lqz1KpVK8+nNQ2YRkLr11B3hvfC0jHo9j9y5IjnW7duXcjjyoJYbVMSmmhtU+xD06ZN88pab4QpAObPn+/sW265xfP17NnT2Rs2bPB89erV88pJSUnO1ukIREQuvPBCZ6MuStdNpyoQEXn00UeltInWNiVFh5olQgghhJBiwsESIYQQQohBmYfhUlNTnT1u3DjPh1XRoRgMn+mp4ZSUFM+np4LnzJnj+Zo2beqVq1ev7mw9LSwi8s9//tPZ+P2tbOM6fCci8tlnnzn7ueee83x6qW1hy+415TkVjCHRxo0bOxtDYIMHD/bKW7dudXZubm7I+lhL9fFZwLKetreWL2PqAv1sLFmyxPNhKEKHYUsq5UFFnt4vzkbF+lkpqQz5JUW0tulbb73llVu0aOGVc3JynK3f2SIiq1atcjY++zosh98d+9D999/v7KpVq3q+UaNGhbyGfm/gu+iNN97wyvg9S4JobVNSdBiGI4QQQggpJhwsEUIIIYQYcLBECCGEEGJwVuEfKVmGDBni7L1793q+AwcOeGW9zNzaYiMrK8srb9y40dkNGzb0fF9++aVX1ikBcDmtLu/atcvz6frg0lb8XvXr1w9Z90h0StFCRkaGV05PT3e2XlYsInLppZd65eeff97ZmPJBa09QE6Y1DRhf1mkF8LyW1gzbRX8vTCuA7d+1a1dnv/DCC54PtRmkcIqjA7F0SvqZwmchkr530UUXObtZs2ae79133w37POWJ7ie4jB/fWcnJyc7GPq3vKfYLnQLgqaee8nyYnkC/3/X9FRF5/fXXnY16Jv3Oxi1UMP1MaWiWSMWEM0uEEEIIIQYcLBFCCCGEGJR5GK5mzZrOxmnx7777zivrUJe1lBzDZ/v373c2hshq1arllTdt2hSyrnppOYYJ9PUPHTrk+XDaWJcxPUF57JpdXPSyeRGR1atXOxtDqe+8845X1u1hZVPHZf3W8nD8rPXc6GcMv4dOQYHtsn37dq+8Y8cOZ2NIQ6dHIP8lkvQAmE1fp6fA/pWZmensRYsWeb5IUgn861//crbePUBE5JxzznG2XjovEjthuAsuuMDZ+B7U0gURvw917NjR8+n+3qVLF8+3Zs0aZ995552er0+fPl553759zsY2veqqq5y9YsUKz4f9PdQ5CSlJOLNECCGEEGLAwRIhhBBCiAEHS4QQQgghBmWuWdJL+XHJt16uKuIvZ9XbkuCxuATY2kIENUxa04L10eB5LK2R1mWJ+MvQGzVq5Pn0tgKxAuqudLu1b9/e8+HWL3qpMd5DawsZDeqQUMOg7z+mnNDH6qXLIiJt2rRxdmHtZOkmSPHBd4Eu47vgtddec/bPf/5zz3f55Zc7G/v3eeed55W7devmbNSdaY1arKaG0Poi1I9Vq1bNK2s9GWr7tC5R3xcRkXPPPbfAz4n4WlIRkV/84hfORm3ZN99842zUpOqUH3gN3PqqVatWzl63bp1UJKz3NG77pTVs+M7U9x+fBUzdoMv4ftfaXnwWtNYs3N+BsoYzS4QQQgghBhwsEUIIIYQYcLBECCGEEGJQ5polrTfYs2eP58PYtNYJ4fYTOkcPxlh1rBR9Vm4XjMdqjQPGUXW+Hsyzg5oGHY9v2bKl54tFzRJqRrQuqUePHp5v+fLlIY+1NEuoS7J0aBib1/Fv1L7o5wb1VJ07dw55XFpamlfWzwrzKhUf3HoG8xzp+52Xl+f5tPZs7ty5nk8/R5gPDfv7smXLQtZHPzfY32MFrScqbHuZdu3aOVvrh0R8vR6+l9977z1now7qRz/6kVfW774FCxaEvL7ulyL+OwTzcaEWS7djPGqWdO6qX/3qV54P779uN8xxp3+jMAeXPg++F7EvWO9pfQ3Mqbh7925n47hA+0R8nRzm4Hruueecjc9mceHMEiGEEEKIAQdLhBBCCCEGZR6G0xS2BFenwMdpOw2GyHA5owanH3EJo0ZPI+I59ZQmLiPHc+rwot4xO1bBJdh6qwS9/F4kf8hOh6wwjYOVukH78LnB6V4dYsBr6LQCuGP65s2bnY07y3/88cdeWYc0ItnGoyKDoVXdbrfffrvnw+l9q5/qFCMYasG20WDqiAYNGjgbw/f6HdOiRQvPp1MQfPvttyGvV96kpqY6G0PgeL/1fVu5cqXn08vzcal+8+bNnY1hEAyZ6DB4p06dPJ9+T+CWVL1793Y2hprwd0Kf94svvpBoxQpfaf75z396ZZ2KR/cDkfxbv1jpdvRvlNVP165d6/mwTfW7D6+hz4vvSKufojxHv5vx9+aWW25x9uOPP+753nrrrZDXCAfOLBFCCCGEGHCwRAghhBBiwMESIYQQQohBqWuWMN6I8VCvMqAn0sdacVxEL1FE7QGeR18TfVrTgLFw/VkdNxaxlyzissxYRGvJRPx4M+rHLO0H6om09gt9+n7jdjL4TOnYPC4X1xoLbO+uXbs6Wy9BFcmvvdJtjG1q6WsqMlYfzsjI8MqoqdHvAtS26VQC2E76mcJUAag11M8cPuP6WHwX/PSnP3X2qFGjJFrR7ylcno3bFOntfi666CLPt3r1amdj/9J9Ae8hag21xga1nDp1hNYHivj9X+vMRPIvM8fvFa1YOscHHnjA2eeff77nmzdvnrPxvYO/Q7rf4PX0exqfDf07ePHFF3s+fN/rdBB4ff1Zqy/iux/1TFpb2r17d8934YUXOvvPf/6z5/v666+dnZWVJZHCmSVCCCGEEAMOlgghhBBCDEo9DIfT4no6HXeaxqlYPa2IU4w6a7O1RBFDNDi9r6cjsT562tBaaonLV/E76ylH3O05VtDfF6dJ9RJVDEnh/V+8eHFY17PCp3h9nFLWoVWcJtbnqV27tudr27ats8855xzPh9O9n3/+ubMxE3hFDsMVNY1Chw4dvDK2m9UX9TWsZ8HKWoz+SEK7N998s7OjOQyn++bOnTs9H76zrCz46enpzs7OzvZ8+j2B9wn7tL7fubm5nk8v+T948KDn02FAHS4Uyf/cWLKP8gT7CT6LGr0cHkNbeuk83m+d0kVE5MCBA87GEKluN0ybokNWn332medr3bp1yPpg6oLt27c7G/uwfsYwXKvrLSLSq1cvZ+uwm4j/+47h+mHDhjlbhzbDJTqfJEIIIYSQKIGDJUIIIYQQAw6WCCGEEEIMSl2zhBoWHZvFuCnGprds2eJsXC5ubWlixX8xhq1jxxjj1NdAn9apzJkzx/NhHFXHYPF7xAqoy9Jo7Q/u1o5Ld/WSfPys1jSghkK3Kcbm8bP6fqO+SbcpPgtax6GXmYrk1yVZy7ArMthPtIYI9SQa1Engu0BrFFELoa+BWhC9zB+vb+mrUDehnz+sW1pamrNxK5TyBL+fvoeoQ8K+YD3Tepsg3CZF61Kwf+vri/gpH5ANGzY4W2sJRUS2bdvmbHyfoi6tcePGIa8RK+i+gc+e1hphm2LaHq1h0u0k4r9TcQsbnS4At4jCLVZ026AuSacHwGcTtcQabEOtZ0MdnJXGBp+jSOHMEiGEEEKIAQdLhBBCCCEGZR6G01NjGIbDzLh6mfn111/v+fSu1DgVp6fMMUsvhgmskJ11nK7rl19+6fkGDhzolXXYwApnRTP6PuL9ttIKfPPNNyHPg1P/+tnAa+gpfGxT3D3eCrXp9sZnU08F43Qzfi9dv3jIyl5S4H2y+OSTT5yNIYMdO3Z4Zd3/IrmGDq1hiMYKu+Mzpp8/KzXEVVddFXbdShtr9wSrX4j4YUgMkehQEN5THZLE4zBko8PnmHlb32+UXOjrY4qPdevWeeXzzjvP2YXt5lCWWCk1+vfv75X1vcHwlf5OGHbSWdBF/HAeZtDWy/Px2dDXwLCnvr94rLWbA0oprHc/vl/1s2H9fqOvuL+9nFkihBBCCDHgYIkQQgghxICDJUIIIYQQg1LXLKGeRMcRMYaIOhG9uzB+Vp8HtQdab4CxYYyjW+fRcVTUMOi6LlmyxPPhEk4dG7eWOoa7NUR5oGPVOqWDiEifPn2cjd/vgw8+8Mp6l3DUqOl7Ye06X1jsOdzl6nh9C/xeuv3x2SAF8/jjj3vlSy+91Nnr16/3fKhT0BoHXIKu9TWot7DSCqBmRWuhLH2LpXXBXeHLE9wKRH8/fGZR36TvG2rEdEoAfGdpfRFuzWGlf8Hz6NQF+D7VKQf0snaR/DpYnQ4ENTz696Wswd+aSZMmOfvKK6/0fPo+4ftM9xPU6FjL8RH9O41tYS3Hx1QGGuynuk/huEBr36y0QCK+3sra+gixfgvCgTNLhBBCCCEGHCwRQgghhBhwsEQIIYQQYlDqmiXcikLHonFrANQtrFixwtkY79axSsz1YekULF0Sxj+1/gGvr/MH4XFWzBWvr2O3Vvy3vNFbmuD3xfT4muXLl3tlvf0J5trQegPUnuj4O8bCrS1ssP31NTCGbWkDcFsHDT5/FQ19/1E30bNnT2fff//9ni83N9fZqDXB+2/lb9HXxGdDfxb7ntVuVt4h1Lrpz6LWqTxBzZLOpYP3At89mBMp1HnwPul2RK0RbiGj88/p7YNE/LaxdKe4FQb+3uhjU1NTPV95apbuu+8+r3zzzTc7G7VGqP3S6HuK98l6vq3+heexNHqodcP3bajzWNsSYR+y8j5Z2kL8jsXdiogzS4QQQgghBhwsEUIIIYQYlHkYTk+b4RSetdO1FWqJJI09Tk1a0726jNP7GEKyfPp7Ylp/Hd6J5jCcvt/WNiU4La6X7orY2w/o81pb2OB0vrVE1qorpiDQz8LatWs9n97pWsQPG+iwRCxh9SF9v/EeYv+ythz45z//6WzcZd5aLmyl2MCwhPbhs6HPg2EhRH8Wl9brJdoYMtL3xwqZlDXYF61wJaYD0SEyfE9nZ2c7W6cREPHfZ3gN3OleSxAwbYy+3/i86f6mt70Syd+n9W8Ktlt5Mn36dK+s76MOT4uIDB061Nk69YqI/7xhaNWSleBn9W8d/g7q5wZ91tZDkaTxsMKAiPXesr4Hbo0TKZxZIoQQQggx4GCJEEIIIcSAgyVCCCGEEINS1yzhtgU63lhYTFuDugitKbA0E4ilS7JS96PWyVoiidojXVdcdhwry851PVGHZm0ZgxoCa9mnvqfYpvqa+NzgsnMdR8e4uaUv0RoK1CFZ+qrCtDDlCX5/jb7HVrqLSPoXar00qKHQzxReA58j3W/wmdKaGvwe+vuj9gbBa2r085CVleX5WrZs6WxLw1HW4HOp32+oycJUAfr74pJr3U/q1q3r+XQakU2bNnk+1DfpNsf65OTkhHWc1k+J5P/OWgtlpf8oa/C3plu3bs7G51u/+/D5tjS3lp4H+5vls7bksn4HrXePpVmyzllQHTTW1mL4rEQKZ5YIIYQQQgw4WCKEEEIIMSj1GFCdOnW8spUl2QrDWVP4OG2np+mslAMF+TV6OhKnJq3w2YYNG7yyXrKKIbpoyvhrob8vhq/0tPGqVas8H34/3eZ473UYBMMZOgyDoUycttbPHC6J1iEEvIY+T2FLwHVGc1yuHk1EEkLTNG3a1Nm4C3qPHj288pAhQ5yt74uIyMaNG52N4Vt9/7F/YxvrMA1m09epIzBMoXeaxzAcLtHesWOHs/E9oe/jv//9b8+nwztW5uuyBsPVVooNDIPpvrBt2zbPp99n+B7UWbELky7oe4q70OvwHoZdmjdv7mzsw/g99DsGw/XlCYaEfvCDH4T8rL7/2Ia6jPfCCplFkh5Ag21qhfOwva1Qv/6dKOy5sVL66GvgO6Sw8F5hcGaJEEIIIcSAgyVCCCGEEAMOlgghhBBCDEpds4Q6AR2bbNWqledbt25dyPNYS4Kt3ZZRF4Nxax1zRd2ELqMuBVMiaFCzpHf/tu5HNKPrjXFqfZ/wfuNyXa1ZimTXd30NjNtrXYqI38Z4DX0ea7m6XgItkr+d9DVwexutm4impeR/+MMfvPIvfvELZ+M2CpGgdXioA9Ltr7fQQLAPWykf9u/f7/n00nJcOq41NP/v//0/zzdhwgSvrLU4uC2R1lvhs6H1XXrJe3mDqRBQI6pBzVB6erqzUb+3evVqZ2Mf0v0C33XW7vHYT/TzqOsi4vd3bCesj342Ud8Si+hnTcRP8RBNqRHKC50qAlNOoJ42UjizRAghhBBiwMESIYQQQohBqYfhcLmetQTdwgpfWUskMXxiXdOatsbz4BJVDS5Xt5ZvW+eJJnQ9MQymp8KbNWvm+XB6X0+3W9/dWtqKIVmrjOfRy0lx6l8ve7eWWYv4oTd8bvR0OC67LmteeeUVZ992222eT/eFSOqJ4RwrS7a+j9jeVtZgDHvrPq3D2iL+M3b//fd7vieffDL/F/gPU6ZM8cp6mh7ro++VlV0eQ7LlCaZq0PcU32crVqzwyh988IGzO3fu7Pn0vcC0Hfr9jmEvLOu+mJeX5/nWr1/v7FmzZnm+v//9787G9sWd5XXYHcMyscLVV1/t7AEDBng+nZrj8OHDng/T1OgyvjOttAIafNehzEE/c1YKAOzfuu7oQ3mMlZ5AP2OXXnqp55szZ44UB84sEUIIIYQYcLBECCGEEGLAwRIhhBBCiEGpa5YwbqljnrgEGNEaB4y/W3oHHePEmCaWtf4CNRU6ro/xX9zGRbN161avrOuH149FzZK1HB7jy3v27PHKehkwal/0s4Gxcd0W+CygFkHrZLCd9HlxKbPW7egl5yIigwcP9srWbuY6rURZa5b08ncRX+OAOgW9lB+fQ0sjhv1N30fcYkA/D5beAfsFap8aN27s7OXLl3u+Tp06SThgiglEfw/8zvqZsrZR0Eu5yxu8h1oXgs/+Z5995pXffPPNAu1ow3rXivjPH76bYoWlS5cWaBObBQsWlOj5OLNECCGEEGLAwRIhhBBCiAEHS4QQQgghBqWuWcJ8HlqngLk1kObNmzsbcz/o+LO1ZQjqJDCOr3MEWbldMBaOWx5odI4QEV/jgxou3H4lWtHfwYr9Y54T3MJGtznmZNL3xspNhXoSrI/WjVhbbGBOHL1VB25ng+2m62dpr8qanTt3euWpU6c6+8orr/R85557rrOj/Tl8/vnnnT18+PCQn8P+rfs0PjeI1rdZ27/ofFwivmYN7395Yukh8ZnFLZo0+H4tC+2Pfvdi39N1x+1OrG1zYmVrKRKdcGaJEEIIIcSAgyVCCCGEEINSD8PhlK2eFsVtQRC9jQGexwqRaXC6Gadp9dSsNb2MPgzTaPbu3euV9bQ9hoXKM2QTCTqNvLU8HEOZiE4XUVjqiHBZuXJliZxHU9iUvQ734P3ALV7KEny+R44cGdZxGBLVYSjcXgRDrfr5xu+u7xPWTT8ra9as8Xw6fBgJ1vOHIbLevXt7Zf1Owe1udCgIr1GzZk1nZ2VlhV/ZUgbTMVjoEHQ0oJ8V671cmJQjVt6vJPrhzBIhhBBCiAEHS4QQQgghBhwsEUIIIYQYlLpmqVq1av4FVex/+/bt5rF6eT5uG4FLhDWoU9KgFkWXcQsArYVCvYXWKRSGjrnjNfD+RCv6PllLsK2l+pEQid7CAtvNOq/+LD4n+Ezp86CmItqX4RcEbu+C5XgEt/iIN44cORKyjM9zSaU80P0C+15Rsd7LOTk55rFas1RS7xRSMeHMEiGEEEKIAQdLhBBCCCEGpR6GwyX2OuxkLfkXEUlOTnY2hjZ05lZcHqoz12LIyFpajNO0Vkbp1NRUZ1vTxCJ+pmhcWt22bduQ9YkmdKjJCsOV1BLkkprCt85bnGl5fT9Kq66EFAeddV/Ef/fhe9AKn0fST0qjL+D7VYNpWvA763Dj4cOHS7ZipELBmSVCCCGEEAMOlgghhBBCDDhYIoQQQggxKHXNEi6x1xoma4m/iMg//vEPZ48aNcrz6e0IcPm9Pi/uWI1Lwg8ePBiyPrqMqQr0UtvCtFcdO3Z0NmoDIklBUJ7o7U60LeLf00hSIaAWIpq0P1g3bH+tf8D7obV2hJQX559/vle2+iZqfTSFvd9KG+v6uN0J6pt0+hlr2xRCCoMzS4QQQgghBhwsEUIIIYQYlHoY7pe//KVXbtiwobM3bNhgHqtDVv369fN8P/7xjws8p4ifSsAKA4r4WW1xCldfH3dFnzZtmlFzn+uuu87ZkSzZjSZ0qgarzoXtAq4p77Abtreepscp+++++y7keXQoVyT/jvWElAeTJk3yyp07d3Z2ixYtwj5PeYThdBjcuv6BAwe88htvvOGVV65c6eysrKwSqh2piHBmiRBCCCHEgIMlQgghhBADDpYIIYQQQgwSgvIWjhBCCCGERDGcWSKEEEIIMeBgiRBCCCHEgIMlQgghhBADDpYIIYQQQgxierCUk5MjCQkJMm7cuBI75/z58yUhIUHmz59fYuck4cM2jT/YpvEH2zT+YJvalPlg6fXXX5eEhARZunRpWV+6TFi9erXce++9kpGRIUlJSZKQkCA5OTnlXa1SJd7bdObMmTJkyBBJS0uTatWqSdu2bWXkyJExk329KMR7m7Zo0UISEhIK/Ne6devyrl6pEO9tKiKyZcsWue666yQ5OVlq1aolP/7xj2X9+vXlXa1SoyK0qYjI9OnT5eKLL5bq1atLcnKyZGRkyNy5c8u0DqW+3UlFY+HChTJp0iTp0KGDtG/fXpYtW1beVSLF5Oc//7k0adJEbr75ZmnevLn8+9//lmeffVY+/PBD+eqrr6Rq1arlXUUSIRMnTpRDhw55f9u4caM8+OCDcuWVV5ZTrUhxOHTokPTt21f2798vv//976VKlSoyYcIE6d27tyxbtkzq1atX3lUkRWDMmDEyduxYyczMlFtvvVVOnDghK1askC1btpRpPThYKmF+9KMfyb59+6RmzZoybtw4DpbigBkzZkifPn28v1100UUybNgwmTp1qtxxxx3lUzFSZAYNGpTvb48++qiIiNx0001lXBtSEjz//POydu1aWbx4sXTt2lVERPr37y8dO3aU8ePHy2OPPVbONSSR8sUXX8jYsWNl/Pjxcu+995ZrXaJSs3T8+HF5+OGH5aKLLpLatWtL9erVpVevXjJv3ryQx0yYMEFSU1OlatWq0rt3b1mxYkW+z2RlZUlmZqbUrVtXkpKSpEuXLvLee+8VWp8jR45IVlaW7Nq1q9DP1q1bN9/mvSS22xQHSiIigwcPFhGRVatWFXp8vBLLbVoQf/3rX6Vly5aSkZFRpOPjgVhu0xkzZkjXrl3dQElEpF27dnLZZZfJ22+/Xejx8Uost+nEiROlUaNGMmLECAmCIN9scFkSlYOlAwcOyMsvvyx9+vSRJ598UsaMGSN5eXnSr1+/AmdqpkyZIpMmTZLhw4fLAw88ICtWrJBLL71UduzY4T7z7bffSo8ePWTVqlVy//33y/jx46V69eoyaNAgmTVrllmfxYsXS/v27eXZZ58t6a9aYYi3Nt2+fbuIiNSvX79Ix8cD8dSmX3/9taxatUpuvPHGiI+NJ2K1TU+fPi3ffPONdOnSJZ+vW7dukp2dLQcPHgzvJsQZsdqmIiKffPKJdO3aVSZNmiQpKSlSs2ZNady4cfn8FgdlzGuvvRaISLBkyZKQnzl58mRw7Ngx72979+4NGjZsGNx2223ubxs2bAhEJKhatWqQm5vr/r5o0aJARIJ7773X/e2yyy4L0tPTg6NHj7q/nT59OsjIyAhat27t/jZv3rxARIJ58+bl+9vo0aMj+q5PP/10ICLBhg0bIjou1qhIbXqG22+/PahcuXKwZs2aIh0f7VS0Nh05cmQgIsHKlSsjPjZWiOc2zcvLC0QkGDt2bD7fc889F4hIkJWVZZ4jFonnNt2zZ08gIkG9evWCGjVqBE8//XQwffr04KqrrgpEJHjxxRfN40uaqJxZqly5spx99tki8v3/GPbs2SMnT56ULl26yFdffZXv84MGDZKmTZu6crdu3aR79+7y4YcfiojInj17ZO7cuXLdddfJwYMHZdeuXbJr1y7ZvXu39OvXT9auXWuKxfr06SNBEMiYMWNK9otWIOKpTf/617/KK6+8IiNHjozblVPhEC9tevr0aZk2bZp07txZ2rdvH9Gx8Uastul3330nIiKJiYn5fElJSd5nKhqx2qZnQm67d++Wl19+WUaNGiXXXXedfPDBB9KhQwenMSwronKwJCIyefJkOf/88yUpKUnq1asnKSkp8sEHH8j+/fvzfbagH6w2bdq4Jfvr1q2TIAjkoYcekpSUFO/f6NGjRURk586dpfp9SHy06T//+U+5/fbbpV+/fvLHP/6xxM8fa8RDm3766aeyZcsWCrv/Qyy26ZkVqceOHcvnO3r0qPeZikgst2mVKlUkMzPT/b1SpUoyZMgQyc3NlU2bNhX7OuESlavh3nzzTbn11ltl0KBB8tvf/lYaNGgglStXlscff1yys7MjPt/p06dFRGTUqFHSr1+/Aj/TqlWrYtWZ2MRDmy5fvlx+9KMfSceOHWXGjBly1llR2X3KjHhoUxGRqVOnSqVKleSGG24o8XPHGrHapnXr1pXExETZtm1bPt+ZvzVp0qTY14lFYrlNk5KSJDk5WSpXruz5GjRoICIie/fulebNmxf7WuEQlW/7GTNmSFpamsycOVMSEhLc38+MWpG1a9fm+9uaNWukRYsWIiKSlpYmIt+PUC+//PKSrzAplFhv0+zsbLnqqqukQYMG8uGHH0qNGjVK/ZrRTqy3qcj3MxHvvPOO9OnTp8L+mGpitU0rVaok6enpBSZnXLRokaSlpVXYVcqx3KadOnWSJUuWyPHjx10oUURk69atIiKSkpJSatfPV58yu1IEnBlFBkHg/rZo0SJZuHBhgZ9/9913vRjp4sWLZdGiRdK/f38R+X4U2qdPH3nppZcK/J9HXl6eWZ/iLkkmsd2m27dvlyuvvFIqVaokf//738u0g0YzsdymZ/jwww9l3759DMH9h1hu08zMTFmyZIk3YFq9erXMnTtXrr322kKPj1diuU2HDBkip06dksmTJ7u/HT16VKZOnSodOnQo0//glNvM0quvvipz5szJ9/cRI0bIwIEDZebMmTJ48GAZMGCAbNiwQV588UXp0KFDgXkWWrVqJT179pS77rpLjh07JhMnTpR69erJfffd5z7z3HPPSc+ePSU9PV1+9rOfSVpamuzYsUMWLlwoubm5snz58pB1Xbx4sfTt21dGjx5dqCht//798qc//UlERD7//HMREXn22WclOTlZkpOT5Z577gnn9sQk8dqmV111laxfv17uu+8+WbBggSxYsMD5GjZsKFdccUUYdyc2idc2PcPUqVMlMTFRfvKTn4T1+XggXtv07rvvlr/85S8yYMAAGTVqlFSpUkWeeeYZadiwoYwcOTL8GxSDxGub3nnnnfLyyy/L8OHDZc2aNdK8eXN54403ZOPGjfL++++Hf4NKgjJdexf8d6ljqH+bN28OTp8+HTz22GNBampqkJiYGHTu3DmYPXt2MGzYsCA1NdWd68xSx6effjoYP3580KxZsyAxMTHo1atXsHz58nzXzs7ODoYOHRo0atQoqFKlStC0adNg4MCBwYwZM9xnirsk+UydCvqn6x5PxHubWt+td+/exbhz0Uu8t2kQBMH+/fuDpKSk4JprrinqbYopKkKbbt68OcjMzAxq1aoV1KhRIxg4cGCwdu3aot6yqKcitOmOHTuCYcOGBXXr1g0SExOD7t27B3PmzCnqLSsyCUGg5uYIIYQQQohHVGqWCCGEEEKiBQ6WCCGEEEIMOFgihBBCCDHgYIkQQgghxICDJUIIIYQQAw6WCCGEEEIMOFgihBBCCDEIO4O33lMmGujbt6+zzzvvPM+3e/duZ8+cOdPznUnZfobExERn16lTx/O9+OKLxa5naVES6bGirU0rOmzT+CMe2rRjx45e+c4773T2ypUrPd9rr73m7KNHj4Z9Ddy2YujQoc7+8MMPPd8333wT9nlLg3hoU7y+/k64Oa7edaJq1aqe78iRI145NzfX2XfffXeJ1620COcanFkihBBCCDHgYIkQQgghxICDJUIIIYQQg7D3hivvGCty6tQpZ+/fv9/z1a5d29kYU61evbpXzsvLc3b9+vU9369//Wtn/+lPfypyXUuDeIibR0KVKlWcvW/fPs83d+5cr9y6dWtn79271/NdfPHFJV+5EqKitWlFIFrb9NZbb/XKAwcO9MpaB7p161bPV61aNWfjO7NVq1bOXr9+vec7++yzvbLux+eee67n0+/0FStWeL569eo5e/bs2Z7vz3/+s1fWGpqSIlrbtDjX1/d/2rRpnq9ly5bOxjY8fvy4V9a/r2+99Zbn++lPf1q0ypYB1CwRQgghhBQTDpYIIYQQQgxiJgynp3dFRFavXu3sNWvWeL5Klf47BkxKSgrpExE5efKks3X4TkRk/vz5zr7mmmsiq3ApE49TwdZ30ikfcCnx2rVrvfLp06edrVNDiPhTyiVVt5IiHtqU+ERTm06fPt3ZuFT/u+++88onTpxwtpY8IJUrV/bKOv3Kjh07PJ8On4mING/e3NkopdiwYUPIa+g+jUvZ8X4vWrTI2b/97W/zf4EiEE1tatG2bVuv/Oqrrzp7y5Ytnk+H1zAdxKBBg5zdtGlTz1erVi2vvGTJkgJtEZEaNWo4Wz9fIiLbtm1z9kMPPSRlDcNwhBBCCCHFhIMlQgghhBADDpYIIYQQQgzC3u6kvLn00ktD+rRGRST/8kYNapbOOuu/twBj81dccUUkVSQREknsf/Dgwc7GWPjnn3/uldPS0pzdqFEjz3fzzTc7+8033yyRuhESjaD2Q2uGdMoUEf89KOIvCcf3qdbboPZk+/btzkY9C/apb7/91tmomdLXwHe2Tgdz4MABz4ffo0ePHs6+8sorPd9HH30k8YzWfYn42k7UiOl2xHfmJ5984mzU7u7Zs8cr//3vf3d2cnKy59NpXNCnU8NEK5xZIoQQQggx4GCJEEIIIcQgZsJwF154oVfW08Q49aqn9HDpOIbsNDilrLPT4lSwdR4SHm3atPHKOnzWsGFDzzdlyhRnT5482fPh9L5uK8z2fc455zj7hhtu8Hx6mnrTpk2eb+fOnfnqT0g0g2GnXbt2ORvfmRhq0ylXUJ5w7NgxZ+N7UYPZ8/E8+j2N19fvV1xmr6+JPsworcOCt912m+eL9zAc/p4dPXrU2XifNNhO+j391Vdfeb5Dhw55ZZ1aQF9PxA+Z6izwIrGR8oQzS4QQQgghBhwsEUIIIYQYcLBECCGEEGIQM5qlFi1aeGW99BHT4et4PMbUMR6rtzuxSE9P98rLly8P67iKgI43o34Il4j+7Gc/czbGtHWbYjtpDRP6MDav2x+1T5bWrEOHDs7WS45F8sfmX3/99ZDnISQaQF2I3hoE+8Hhw4e9sn6n4jtU93GrP+FycNSPalCzoq+P+hpdH3wXYD+tWbOms/E3JN5JTU31ytWrV3c23id9j3Nzcz3f5s2bna23LBHJnx5Cv9Px/W5hPRvRAmeWCCGEEEIMOFgihBBCCDGImTBc7dq1vbKefsWpYD2liztd407YekrZytpct27d8CtbwbDuW9euXUP6cHm+blOcXtfTtBh2xevrKWU8jz4Ws//q5wZTDuAO3hdccIGzGZIl0YJOjaFDUCJ+6gAMkR08eNAr6z5lZfC2KCzdivZjH9ZhQR0+RB+G4C3wsykpKc7GjObxgE7/IOK/w7Zu3er5dFtgSEy3P7Yplq2UBFbIVrdFtMKZJUIIIYQQAw6WCCGEEEIMOFgihBBCCDGIGc0Sxjt1GWPRWl/0xRdfeD6M4zdu3NjZuHxWE0lsnPwXfX9F/CWruOWCXmqKKR10e+NxVjoIPI+Ov+NxVkwdl9pqbQg1SyRa0Mvjt2zZ4vl0KgHUYGKKD60nxH4SrmYJ+xPqknTfxM/qJenr168PeR7UXqG2VYNanHjXLGVlZXllvVUJ6tC+++47Z6PWydpeBtGfxfe0bmNsiz179pjnjQY4s0QIIYQQYsDBEiGEEEKIAQdLhBBCCCEGMatZ0vlyMI6q47H/+Mc/PN/AgQO9so55W/mCMJ8ECQ9sG30fUT+mtzux9A6oH7PywKBmSeuisG5am4F5PzCOb+mbCCkvmjZt6mzMeaNzKaGWEHMZab0T9iHr3Wv1i0jer7qf4nti7969zsbv0ahRI6+stUioS8KtOuIdnVupWbNmnk9riHCbHN3+qF/TW6iI+O2Iz4I+L/ruv/9+s+7RAEcAhBBCCCEGHCwRQgghhBjETBgOl4iGS3Z2tlfGqWAdwrGmiTE9PAkPnNLVoYE6dep4vo0bNzobp3t16M2aJsZjsU31eayQAS5Bxu0gMPRHvse6L9ZWCJGAO5/fd999zv7ss88838cffxyybiVVH727u36Gy4OePXs6G8NXut/gcvxvvvnGK1977bUhfbgkXKP7FPYv7NPWMnO9lB2/h/ZhyoP69et7ZdzeStOpUydnY4qZeERLCVBWYKXG0WFXDJdiX9RtrtupoGM1+A6PRjizRAghhBBiwMESIYQQQogBB0uEEEIIIQYxo1nCJaq6bKVg37x5s1eOJAWBBjUrpGAwho1aM53WHmPYWpuAWiMd/9ZtVhBa/4BaCL1EFjUrOm6PMfxjx455ZfyepGAsXZC+h7idDC4Bb9OmjbPPO+88z3fLLbc4+4orrvB8WrNUUlx++eVeefDgwc4ePnx4iV8vEmbMmOHsI0eOeL6MjAxn4/YSn3zyiVceOnSos602RB2SxtKAithL0vWy/u3bt3s+rVOaPXu257v77ru9st7WBTU05557rlm/eENvhWO1G/5Ghvtbi+e1Uk5gygGdqiU3N9e8RnnBmSVCCCGEEAMOlgghhBBCDGImDIfoEI61JBGXyOLUoDXFqKeRcUqbFAxOryI6My9mDdZhOKstrCXIIv50L4bs9GetrNxYN2x/TF9AvscK2WD4rEePHs7GZf2Ymbl169bOxpCdThdghd2KkypAh3N0vUVEdu/e7ezyTinx6aefFmgXRseOHb2yft4xlB7urgdWqEfE74sYItMhWryG7psYhps+fXrI82zbts3zHT582KxfvFGvXj1n6wztiCV5wbawPovvZe1DeURhvxvRAGeWCCGEEEIMOFgihBBCCDHgYIkQQgghxCAuNEvWckZcImvFXJk6oPig1geX4Ot208v4Rfz7j3oHawsTjI1rP372wIEDzsa4udYhYWoAjPHjdiikYHQKAFxyrzVEF154oXme3r17Oxv1TF999ZWzUc+0cOFCZ69bt868htZU6XQEIr6eTmuURHwtCNatrLH0JRY7d+70yvpYfC9a2wSFqktBx1kpQHR/x/eE7u979+71fPi+r8hYWk7rs5H8RuI7VJ8H2zvc3+xohTNLhBBCCCEGHCwRQgghhBjETBjOmrazfDgtb2UnjWTamhQM7vqN91SH5XB6XbeVtQTbWq6K18Tr62ljTCOhp4l1BmGR/GFBnXYAQ4+4DDre0eErDKfpcNYbb7zh+VJTU52Ny/oxzcDzzz8f0qezdqNvyJAhzsZnCnez16F2y4dL0PVnyzsMZ73DdBgGn2ed0kNEJCsrq8DjCjo2XDAspN/FGM7RfQjTdOi0BjqsXhD6vHhv9PXj8d2P91R/R+vdV9j71bpGuCl90MfUAYQQQgghMQ4HS4QQQgghBhwsEUIIIYQYxIxmCQk3xoyfw93jtRbBirGS8GjSpIlXRi2EBtMKaG0Exsn1FguFaSZ0HB3bVMfqUYuhj8PtTXDLB/29MM1ArGiWtGZIbyci4n9fvVu5SH7tj9ae4Xm01kgvsRcRueSSS5yttywREXnqqae88oQJE5yNffiOO+5wtk4xICLSuXPnAuspkj8diE4JgJ/VYHtb9zGasN6Z+LzrfmI9+9hPra2G8Pq6H+PWQ/v27XN2gwYNPF8k/ctKORLvYD9FfZHG0ixpH7578ZxWmgFrK5RYSMXC0QEhhBBCiAEHS4QQQgghBhwsEUIIIYQYxIxmyUqdHonWyNLJWOngSXig1gP1JXr7C9SppKSkOBvbG9tNY+XOQp8uY0x9+/btzt61a5fna9q0qVfWzxzmCMnLywtZ19IGdQqXXXaZs1F7pLeGwON02dIoifhaH/zsF1984ew2bdp4Pq31wa1QUBekwfM89NBDzt66davn0zmRsN54jY4dO4a8pv5emINJP+PoixUw747WpqBmydKl6P6F71MrHxpqX/Q9Ra2R1ldhm1r5g4qaHypWwWexJHIKWnomEVt3qn14HOa1i0Y4s0QIIYQQYsDBEiGEEEKIQcyE4XAJelHDZ5Gkea9oS01LAgzD4P3WW5pgaE1PG+Oybn0enLLHsrWNgi7jFht6W4XCUgfobRbq1q3r+XJycqS80EvlRfzpbWtHdgw76m1r8D5h6EOHr/BePPLII87G7U4mTpzobL38X0Rk8ODBXtna/kbXR4f2RPxQm5UOQMR/5vA7h/qciH9f0Vfe258UFWtrFGsJun5nWtubiNjhch3aPnz4sOfTy8wpnQgNLse3tjvR7zfrnmKbWluhRPLZ5OTkkL5ogTNLhBBCCCEGHCwRQgghhBhwsEQIIYQQYhAzmiUrdUBJnRdjqhjXJYWD2pL9+/d7Zd1uqG/R9xvbwtJJINZndXujLkXrWzBuj3ofXddo2iYH0wPoJdh6Gb2IrxFDTZb+LPpQQ6L1JVrrhKCeSGu7Zs2a5flwaxT9XOFzo7VImKpCY+mQRPw2xvPoY/E8um46jYJI/u1XohXUjGj9Hra3tQTd0iEhli5K1wdTceg0Hrg8XmsJsT4VDeyL+j1l6ccsrRG2UyRbmljn0WljopXoecsTQgghhEQhHCwRQgghhBjETBgOQ2I61GItKy7sPBqcNixs2p58j55exWzWGJbQn61atarnCzfsaWUbRj+G5PRn8TgrHYW1fDqasjZjqEv3DQwJ6WXuGKLTx2E/sMrYF3VoE0Nruq4bN240rxHusv7i9Fm9zN96p6BPt380PQuIFZJq1aqVV9bPP4Zsirpc3wrZIVZKF93GderU8XxbtmzxyoWFAuMZDK1afUO/z/B30EqhY7Upnke/l/E4THMQjXBmiRBCCCHEgIMlQgghhBADDpYIIYQQQgxiRrOEy0cj0Slp9u3b55WtnZArcrw7EvR9++677zwfblOj7ykuAdfbn2BMW5exXfBZ0G2KPn0e1F7oLU7wWbD0VZGkNShtnnrqKa+sv3/z5s09X+vWrUOeRy87xnuI2iOtS9Lb2YjYuqhQ5yiorMFl/Tp1ANbV0lNFsqWJ/l74HfVydfyOv/71r0NeI5o455xzvLKlPdFYeqbCdH8a7O+6bXQaAxG/n6JGb8WKFV65IqcOwJQf+v2KPg22t2439OG7T/cxfPdb73Crv0cLnFkihBBCCDHgYIkQQgghxCB64geFgFO4Rc2ajFOD1jJYhuHCIykpydk47Y330JrS1WC76LK1lBXrYLWhtURWT1kXVNbT2FbW6PJGhzPWrVvn+bBM4hurL2DmdyvbsxXC0RTWT62wjC7ju0CHwJs0aVKsOsQz1vvNek/j/da+wjJ462viebQP24VhOEIIIYSQGIeDJUIIIYQQAw6WCCGEEEIMYkazZG2FEUlcGrUnFtG0m3w0Y6VxwHbTqRsaNmwY8jhsJ629wXPqpeMivoYIP6vLqDXSy2BR24axeq2VKOr2D4SUJdZ7EtNIWO9bS99i6WKs1Czo0+e1troqTC9YkXWnVkoTSwOMx+l3b2G/iVb76zJeH7etiUY4GiCEEEIIMeBgiRBCCCHEgIMlQgghhBCDmNEsHT582Cvr+GckKe1xiwN9rJVfgoRG58jA+4v3cNeuXc7GrVH0thGYy8XK34HtrzVU6NPXRC2EvoauZ0H10eVo2u6EkKJgbT2EmkTdb6x3b2H6Fu0PVyMl4m89VNg1dP0qWs4l3KIpXL2u1abYFta2VFZ+LG53QgghhBASZ3CwRAghhBBiEDPxA73TtIgdPrPAaVumByg+1v23lqj26tXL8+lp2urVq4d9zkjqY31O1+2DDz7wfKtWrfLKDNGSeMIKvWB/s7YeimT7KKsP6dA2Xr+w/k++p379+l5Zp1mwQpIoK9DbWR08eNDz4e+nlidg2M9q02rVqoWsT7TAkQIhhBBCiAEHS4QQQgghBhwsEUIIIYQYxIxmae/evSVyHkxBoMEYOrexCA8rFq3j3SIimzZtcvajjz7q+axlvrptcCkz6i0sLYRuU4yp67j5li1bPF+LFi28so7NV7QlySQ2wH4RyXMabkqASNK2WNew0rZYOlPU5SAVuW926NDBK+vfPtQlWdud6Law9Ewidpvqa+C7V6d0sbavKk84s0QIIYQQYsDBEiGEEEKIQcyG4XQqgUiWce/Zs8crW5nAo2X6L9rRU7PYFlZm3n379pVWlYqEFerF76XDjdZ3JCQW2L17t1euVauWs60szWXx7FupA2JhyXl5odtQxH+/YZvqnRdwFwadLgB/E/WuCwUdq9HPCj43tWvXdvagQYM83/Tp00OesyzhzBIhhBBCiAEHS4QQQgghBhwsEUIIIYQYxIxmCbc70Uu3I1m+iksdLVJSUsL+bEVGx5vx/kZyv/XS0vJY8quvjzoJjMVrPcDRo0dLt2KElDJ9+/b1ylrDhHo9/V7EfqI/iz7s09Z7W7/fd+3aFfK41q1bhzxHRefLL7/0yldffbWzUaOm3+GYjkFrSzEdQJs2bbzy8uXLnV2vXj3Pt3nzZmdj+pcdO3Y4G3VQ0QJnlgghhBBCDDhYIoQQQggxiJkw3CeffOKVdQoAnFLEsmbWrFleediwYc7GKcaPPvoo4npWRHT21Z07d3o+K2M6oqfpyyMMZ6Wg0N9RxA8NYDoKQqKBSPpQv379vHJqaqqzUY6gQzY1atQI6cNMzFYKAmT//v3O3rp1q+fTy9e3b98e8hwifmg9khQz8cDjjz/ulefOnetsTCtgtakG34OI3vkAJRi6HTEre05OjrMPHTpkXqO84MwSIYQQQogBB0uEEEIIIQYcLBFCCCGEGCQEFXlbZkIIIYSQQuDMEiGEEEKIAQdLhBBCCCEGHCwRQgghhBhwsEQIIYQQYsDBEiGEEEKIAQdLhBBCCCEGHCwRQgghhBhwsEQIIYQQYsDBEiGEEEKIwf8HDis3A5Zf6J4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 600x400 with 15 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "subplots_grid = (3, 5)\n",
    "n_elements_in_batch = subplots_grid[0] * subplots_grid[1]\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=n_elements_in_batch, shuffle=True)\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Label batch shape: {train_labels.size()}\")\n",
    "\n",
    "fig, axes = plt.subplots(*subplots_grid, figsize=(6, 4))\n",
    "for batch_idx in range(n_elements_in_batch):\n",
    "    i, j = np.unravel_index(batch_idx, subplots_grid)\n",
    "    axes[i, j].imshow(train_features[batch_idx].squeeze(), cmap=\"gray\")\n",
    "    axes[i, j].set_title(f\"Label: {train_labels[batch_idx]}\")\n",
    "    axes[i, j].axis(\"off\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2000d067",
   "metadata": {},
   "source": [
    "## Transforms\n",
    "This allows to perform some manipulation of the data to make it suitable for training. `Dataset` has two parameters:\n",
    "- `transform` to modify features, or input data.\n",
    "- `target_transform` to modify labels.\n",
    "\n",
    "The [torchvision.transforms](https://pytorch.org/vision/stable/transforms.html) offers a variety of helpers out of box.\n",
    "\n",
    "For example, to convert the labels of FashionMNIST dataset to one-hot encoded tensors, one could do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0c401249",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Lambda\n",
    "from torch.nn.functional import one_hot\n",
    "\n",
    "ds = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=ToTensor(),\n",
    "    target_transform=Lambda(lambda y: one_hot(torch.tensor(y), num_classes=10).float())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9c9f02",
   "metadata": {},
   "source": [
    "## Build the neural network\n",
    "First step is to define the neural network architechture. This is done by subclassing `nn.Module` followed by definition and initialization of the neural network layers in `__init__`. Every `nn.Module` subclass implements the opertations on input in `forward` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "66203967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # converts 2D image to 1D vector\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "    \n",
    "model = NeuralNetwork()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bb08e3",
   "metadata": {},
   "source": [
    "Internally the model consists of:\n",
    "- The flattening layer, that converts a grayscale image to a flattened vector, which length is 28*28=784.\n",
    "- Fully-connected layers:\n",
    "  - the first layer accepts 784 features and outputs 512. Internally, it stores a bias vector `b` of shape `[512]`, and the weight matrix `W` of shape `[512, 784]`. The output of the layer is computed as: `y = W @ x.T + b`, or `y = x @ W.T + b`.\n",
    "  - the output of the first layer is passed through a non-linear activation function (`ReLU`). Non-linear activations allow the NN to capture complex dependencies between inputs and outputs.\n",
    "  - the second layer is a hidden layer that accepts 512 input features and maps them to 512 output features: it stores a bias vector of shape `[512]` and the weight matrix `W` of shape `[512, 512]`.\n",
    "  - the output of the second layer is passed through a non-linear activation function once again.\n",
    "  - The final layer maps 512 input features to 10 raw, unnormalized scores. Its `W` matrix size is `[10, 512]` and bias vector size is 10.\n",
    "\n",
    "To check manually computed weights and biases sizes we can iterate over parameters and print their sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5cd23a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: linear_relu_stack.0.weight | Size: torch.Size([512, 784]) | Values : tensor([[ 0.0063,  0.0163, -0.0114,  ..., -0.0334, -0.0219,  0.0322],\n",
      "        [ 0.0178, -0.0129,  0.0207,  ..., -0.0290, -0.0264,  0.0250]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.0.bias | Size: torch.Size([512]) | Values : tensor([-0.0331, -0.0257], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.weight | Size: torch.Size([512, 512]) | Values : tensor([[-0.0266,  0.0132,  0.0264,  ..., -0.0158,  0.0086, -0.0355],\n",
      "        [ 0.0155, -0.0433, -0.0386,  ..., -0.0198,  0.0349, -0.0382]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.2.bias | Size: torch.Size([512]) | Values : tensor([-0.0146,  0.0129], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.weight | Size: torch.Size([10, 512]) | Values : tensor([[-0.0067,  0.0228,  0.0264,  ..., -0.0280,  0.0048,  0.0036],\n",
      "        [-0.0194, -0.0232,  0.0125,  ...,  0.0013,  0.0114,  0.0278]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: linear_relu_stack.4.bias | Size: torch.Size([10]) | Values : tensor([0.0219, 0.0210], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61898e0",
   "metadata": {},
   "source": [
    "Now, if we want to use the model, we feed in our data and get output. The model returns 2-dimensional tensor `[batch_size, num_classes]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b76992f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0120,  0.0271, -0.0188,  0.0150, -0.0656,  0.0590,  0.1259, -0.0392,\n",
      "         -0.0789,  0.0629]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.rand(1, 28, 28)\n",
    "logits = model(X)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1be039",
   "metadata": {},
   "source": [
    "Now, if we want convert our raw scores from the last linear layer into probabilities we can use softmax function:\n",
    "$$\n",
    "p = \\frac{e^{z_i}}{\\sum_j{e^{z_j}}}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "26ffd2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.10002154 0.10154515 0.09699149 0.10033008 0.09255714 0.10484016\n",
      "  0.11208949 0.09503733 0.09133658 0.10525102]]\n",
      "tensor([[0.1000, 0.1015, 0.0970, 0.1003, 0.0926, 0.1048, 0.1121, 0.0950, 0.0913,\n",
      "         0.1053]], grad_fn=<SoftmaxBackward0>)\n",
      "Predicted class: tensor([6])\n"
     ]
    }
   ],
   "source": [
    "logits_np = logits.detach().numpy()\n",
    "exp = np.exp(logits_np - logits_np.max())\n",
    "probs = exp / exp.sum(axis=1, keepdims=True)\n",
    "print(probs)\n",
    "\n",
    "# or simply use nn.Softmax, dimension parameter indicates which dimension softmax would be computed along\n",
    "pred_probab = nn.Softmax(dim=1)(logits)\n",
    "print(pred_probab)\n",
    "\n",
    "y_pred = pred_probab.argmax(1)\n",
    "print(f\"Predicted class: {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9b693",
   "metadata": {},
   "source": [
    "## Automatic differentation with `torch.autograd`\n",
    "The training of neural networks is performed using forward and back-propogation.\n",
    "- On a forward pass the inputs are used to calculate the loss function.\n",
    "- On a backward pass the model parameters (weights and biases) are adjusted according to the gradient of the loss function w.r.t. the given parameters.\n",
    "\n",
    "To compute those gradients, PyTorch has a built-in differnetation engine `torch.autograd`.\n",
    "\n",
    "Let's play around with the simplest one-layer NN. Input is `x`, weights are `w` and biases are `b`.\n",
    "\n",
    "**Notes**:\n",
    "- the loss is tied to the NN architechture, so it updates the network parameters in-place after different methods are called on it.\n",
    "- gradients are only available for nodes, which have `required_grad` property set to `True`.\n",
    "- we can only perform gradient calculations once on a graph. If several calls are desired, `retain_graph` flag should be set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0c6c7bd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2008888721466064\n",
      "Gradient w.r.t. weights: tensor([[-0.2073, -0.1949, -0.0758],\n",
      "        [ 0.1192,  0.1120,  0.0435],\n",
      "        [ 0.0007,  0.0006,  0.0002],\n",
      "        [ 0.0992,  0.0932,  0.0362],\n",
      "        [ 0.2001,  0.1881,  0.0731]])\n",
      "Gradient w.r.t. bias: tensor([0.3249, 0.3054, 0.1187])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(5)\n",
    "y = torch.zeros(3)\n",
    "\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "z = x @ w + b\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "print(f\"Loss: {loss}\")\n",
    "\n",
    "# to compute gradients a call to loss.backward() is required\n",
    "loss.backward()\n",
    "print(f\"Gradient w.r.t. weights: {w.grad}\") \n",
    "print(f\"Gradient w.r.t. bias: {b.grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5658c",
   "metadata": {},
   "source": [
    "If we have fully trained network and just want to do inference (forward pass), we usually don't need any gradients computation at all.\n",
    "Switching off gradient functionality can be achieved by doing computations inside `torch.no_grad()` block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d913b6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z = torch.matmul(x, w)+b\n",
    "print(z.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2850d",
   "metadata": {},
   "source": [
    "### Directed Acyclic Graphs\n",
    "During computations, autograd keeps all the data and operations in a directed acyclic graph (DAG), where:\n",
    "- the input tensors are leaves;\n",
    "- the output tensors are roots (we call `.backward` on it).\n",
    "\n",
    "*Note*: for forward pass, the inputs are roots, but outputs are leaves.\n",
    "\n",
    "On a forwards pass `autograd`:\n",
    "- runs the operations and computes the resulting tensor;\n",
    "- snapshots the recipe for how to do backward pass (records the graph and required intermediate values).\n",
    "\n",
    "One a backward pass:\n",
    "- computes the gradients from each `.grad_fn`;\n",
    "- accumulates them in the respective tensor's `.grad` attribute;\n",
    "- propagates gradients from loss to all the leaf tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db10c02",
   "metadata": {},
   "source": [
    "## Optimizing model parameters\n",
    "Now let's train the model, validate and test it.\n",
    "Training is performed iteratively:\n",
    "- model makes a guess about the output given current weights, biases and inputs;\n",
    "- then it calculates the loss;\n",
    "- propagates this loss back using partial derivatives.\n",
    "\n",
    "The training process can be tuned by setting hyperparameters. They impact the model training and convergence rate. The typical hyperparameters are:\n",
    "- number of epoches: the number of times to iterate over the dataset;\n",
    "- batch size: the number of data samples, propagated through the network before backpropagating;\n",
    "- learning rate - how much to update model parameters at each epoch.\n",
    "\n",
    "Hyperparameters tuning can drastically effect the results. To test multiple of them [**Ray Tune**](https://docs.pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) can be used. It includes the latest hyperparameter search algorithms and supports distributed training.\n",
    "\n",
    "### Loss function\n",
    "**Loss function** measures the degree of dissimilarity between the value, predicted by the network and the target value.\n",
    "Loss function is the function we want to minimize. The common loss functions are:\n",
    "- Mean Square Error for regression;\n",
    "- Negative Log Likelihood for classification;\n",
    "- Cross Entropy Loss, that combines `nn.LogSoftmax` and `nn.NLLLoss`.\n",
    "\n",
    "### Optimizer\n",
    "An optimizer's goal is to adjust model parameters to minimize the target loss function. There are various optimization algorithms. All the optimization logic is encapsulated in `optimizer` object.\n",
    "\n",
    "#### Optimization loop\n",
    "An epoch is one cycle of the optimization loop. Each epoch consits of two steps:\n",
    "- the train loop - iterate over the training dataset and try to converge to optimal parameters;\n",
    "- the validation loop - iterate over the test dataset to check if model performance is improving.\n",
    "\n",
    "Inside the loop several things happen:\n",
    "- `optimizer.zero_grad()` resets the gradients, because by default they add up;\n",
    "- backpropagation of the prediction loss with a call to `loss.backward()`;\n",
    "- once all gradients are calculated, the call to `optimizer.step()` adjusts the parameters by the gradients collected in the backward pass."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866761b5",
   "metadata": {},
   "source": [
    "Now it's time to define train and test loops to train the model and evaluate its performance.\n",
    "Let's compare two NN models: the first one is standard, that flattens an image and has three fully-connected layers. The second one uses embeddings and cosine classifiaer to calculate logits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d55f5ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NeuralNetworkCosineHead(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (backbone): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (head): Linear(in_features=512, out_features=10, bias=False)\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.310842  [   64/60000]\n",
      "loss: 1.469005  [ 6464/60000]\n",
      "loss: 0.909483  [12864/60000]\n",
      "loss: 1.001691  [19264/60000]\n",
      "loss: 0.758213  [25664/60000]\n",
      "loss: 0.790737  [32064/60000]\n",
      "loss: 0.749348  [38464/60000]\n",
      "loss: 0.673423  [44864/60000]\n",
      "loss: 0.685838  [51264/60000]\n",
      "loss: 0.668831  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.4%, Avg loss: 0.625605 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.531847  [   64/60000]\n",
      "loss: 0.651578  [ 6464/60000]\n",
      "loss: 0.377852  [12864/60000]\n",
      "loss: 0.681120  [19264/60000]\n",
      "loss: 0.555134  [25664/60000]\n",
      "loss: 0.603500  [32064/60000]\n",
      "loss: 0.583237  [38464/60000]\n",
      "loss: 0.578776  [44864/60000]\n",
      "loss: 0.605892  [51264/60000]\n",
      "loss: 0.553525  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 82.2%, Avg loss: 0.527449 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.416649  [   64/60000]\n",
      "loss: 0.544327  [ 6464/60000]\n",
      "loss: 0.313935  [12864/60000]\n",
      "loss: 0.585359  [19264/60000]\n",
      "loss: 0.480906  [25664/60000]\n",
      "loss: 0.538497  [32064/60000]\n",
      "loss: 0.509261  [38464/60000]\n",
      "loss: 0.543764  [44864/60000]\n",
      "loss: 0.566779  [51264/60000]\n",
      "loss: 0.500396  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.3%, Avg loss: 0.484421 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.364167  [   64/60000]\n",
      "loss: 0.493786  [ 6464/60000]\n",
      "loss: 0.286376  [12864/60000]\n",
      "loss: 0.526898  [19264/60000]\n",
      "loss: 0.429943  [25664/60000]\n",
      "loss: 0.502305  [32064/60000]\n",
      "loss: 0.461340  [38464/60000]\n",
      "loss: 0.519215  [44864/60000]\n",
      "loss: 0.539848  [51264/60000]\n",
      "loss: 0.467646  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.9%, Avg loss: 0.458235 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.328204  [   64/60000]\n",
      "loss: 0.459587  [ 6464/60000]\n",
      "loss: 0.269931  [12864/60000]\n",
      "loss: 0.480626  [19264/60000]\n",
      "loss: 0.392029  [25664/60000]\n",
      "loss: 0.476772  [32064/60000]\n",
      "loss: 0.430706  [38464/60000]\n",
      "loss: 0.500934  [44864/60000]\n",
      "loss: 0.513572  [51264/60000]\n",
      "loss: 0.446072  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.5%, Avg loss: 0.439630 \n",
      "\n",
      "NeuralNetworkWithEmbeddingsDotProductHead(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (backbone): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "  )\n",
      "  (proj): Linear(in_features=512, out_features=16, bias=True)\n",
      "  (class_emb): Embedding(10, 16)\n",
      ")\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 5.076449  [   64/60000]\n",
      "loss: 0.828677  [ 6464/60000]\n",
      "loss: 0.485279  [12864/60000]\n",
      "loss: 0.754989  [19264/60000]\n",
      "loss: 0.662078  [25664/60000]\n",
      "loss: 0.635908  [32064/60000]\n",
      "loss: 0.668154  [38464/60000]\n",
      "loss: 0.616959  [44864/60000]\n",
      "loss: 0.645058  [51264/60000]\n",
      "loss: 0.450713  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 75.4%, Avg loss: 0.695899 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.566858  [   64/60000]\n",
      "loss: 0.488637  [ 6464/60000]\n",
      "loss: 0.312090  [12864/60000]\n",
      "loss: 0.531822  [19264/60000]\n",
      "loss: 0.516915  [25664/60000]\n",
      "loss: 0.536400  [32064/60000]\n",
      "loss: 0.541884  [38464/60000]\n",
      "loss: 0.557266  [44864/60000]\n",
      "loss: 0.562180  [51264/60000]\n",
      "loss: 0.401881  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.2%, Avg loss: 0.567306 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.430695  [   64/60000]\n",
      "loss: 0.422626  [ 6464/60000]\n",
      "loss: 0.277761  [12864/60000]\n",
      "loss: 0.467444  [19264/60000]\n",
      "loss: 0.461435  [25664/60000]\n",
      "loss: 0.495174  [32064/60000]\n",
      "loss: 0.488703  [38464/60000]\n",
      "loss: 0.510982  [44864/60000]\n",
      "loss: 0.502928  [51264/60000]\n",
      "loss: 0.376506  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 81.3%, Avg loss: 0.506606 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.357333  [   64/60000]\n",
      "loss: 0.391268  [ 6464/60000]\n",
      "loss: 0.255522  [12864/60000]\n",
      "loss: 0.432053  [19264/60000]\n",
      "loss: 0.414748  [25664/60000]\n",
      "loss: 0.469247  [32064/60000]\n",
      "loss: 0.452161  [38464/60000]\n",
      "loss: 0.478659  [44864/60000]\n",
      "loss: 0.460553  [51264/60000]\n",
      "loss: 0.362731  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 83.1%, Avg loss: 0.468808 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.316708  [   64/60000]\n",
      "loss: 0.368740  [ 6464/60000]\n",
      "loss: 0.240839  [12864/60000]\n",
      "loss: 0.407876  [19264/60000]\n",
      "loss: 0.390261  [25664/60000]\n",
      "loss: 0.451867  [32064/60000]\n",
      "loss: 0.433741  [38464/60000]\n",
      "loss: 0.453199  [44864/60000]\n",
      "loss: 0.435038  [51264/60000]\n",
      "loss: 0.347449  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 84.1%, Avg loss: 0.440675 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer, batch_size=64):\n",
    "    size = len(dataloader.dataset)\n",
    "    # Set the model to training mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * batch_size + len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
    "    # Unnecessary in this situation but added for best practices\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
    "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class NeuralNetworkCosineHead(nn.Module):\n",
    "    def __init__(self, scale=10.0):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(28*28,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,512),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.head = nn.Linear(512,10, bias=False)\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        f = F.normalize(self.backbone(x), dim=-1)\n",
    "        W = F.normalize(self.head.weight, dim=-1)\n",
    "        logits = f @ W.T * 10.0\n",
    "\n",
    "        return logits\n",
    "\n",
    "class NeuralNetworkWithEmbeddingsDotProductHead(nn.Module):\n",
    "    def __init__(self, emb_dim=16, scale=10.0):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten() # converts 2D image to 1D vector\n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.proj = nn.Linear(512, emb_dim)\n",
    "        self.class_emb = nn.Embedding(10, emb_dim)\n",
    "        self.scale = scale\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        h = self.backbone(x)\n",
    "        f = self.proj(h)\n",
    "\n",
    "        W = self.class_emb.weight\n",
    "        logits = f @ W.T * self.scale \n",
    "\n",
    "        return logits\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=64)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64)\n",
    "\n",
    "model_classic = NeuralNetworkCosineHead()\n",
    "model_with_embeddings = NeuralNetworkWithEmbeddingsDotProductHead()\n",
    "\n",
    "for model in [model_classic, model_with_embeddings]:\n",
    "    print(model)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "    epochs = 5\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        test_loop(test_dataloader, model, loss_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cef9aa0",
   "metadata": {},
   "source": [
    "## Save and load the model\n",
    "PyTorch models can be saved and loaded. The learned parameters are stored in an internal state dicitonary, called `state_dict` and can be saved using `torch.save` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fb8f76c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model_with_embeddings.state_dict(), \"model_with_embeddings.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8debd1dc",
   "metadata": {},
   "source": [
    "To load model weights, the instance of the same model needs to be created and then populated with `load_state_dict()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fe7ec54d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetworkWithEmbeddingsDotProductHead(\n",
       "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "  (backbone): Sequential(\n",
       "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (proj): Linear(in_features=512, out_features=16, bias=True)\n",
       "  (class_emb): Embedding(10, 16)\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_copy = NeuralNetworkWithEmbeddingsDotProductHead()\n",
    "model_copy.load_state_dict(torch.load(\"model_with_embeddings.pth\", weights_only=False))\n",
    "model_copy.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6252d4c0",
   "metadata": {},
   "source": [
    "If we want to save the whole model, with its structure, we need to call `save` on the object instance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-ai-cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
